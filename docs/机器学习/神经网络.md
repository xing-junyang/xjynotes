# 神经网络

神经网络是利用大量节点（神经元）之间的连接关系来进行计算的模型。神经网络的基本结构是由多个神经元组成的网络，每个神经元接收来自其他神经元的输入信号，并产生输出信号。神经网络的学习过程是通过调整神经元之间的连接权重来实现的。

::: tip 仿生学

<div style="display: flex; flex-direction: row; justify-content: space-between; width: 99%; margin: 2% 0.5%">
    <p style="width: 65%">
神经网络是一种仿生学的模型，它模拟了生物神经元之间的连接方式。在生物神经元中，每个神经元都有一个细胞体，细胞体上有许多树突，树突接收来自其他神经元的信号，然后将信号传递给细胞体。细胞体会根据接收到的信号的强度来决定是否激活，如果激活了，细胞体就会向轴突发送信号。轴突是一个长的细胞突，它会将信号传递给其他神经元的树突。
    </p>
    <img src="/image/Blausen_0657_MultipolarNeuron.png" style="width: 33%;" alt="Neuron">
</div>

可以带着这个问题思考：**神经网络和生物神经元的相似之处在哪里？**

::: 

## MP 神经元的基本结构

对于每个神经元，我们可以用一个 MP 神经元来模拟。MP 神经元的基本结构如下：

- **输入** $X = [x_1, x_2, x_3, \cdots]$；
- **权重** $W = [w_1, w_2, w_3, \cdots]$；
- **激活函数** $f(net)$，其中 $net = \sum_{i=1}^n w_i x_i + b$；
- **偏置单元** $b$，也称为阈值。

我们可以利用简单的 MP 神经元来实现逻辑运算（使用**阶跃函数**作为激活函数）：

- **与门**：令 $w_1 = 1, w_2 = 1, b = -1.5$；
- **或门**：令 $w_1 = 1, w_2 = 1, b = -1$；
- **非门**：令 $w_1 = -1, b = 0.5$。

### 激活函数

激活函数决定了单个神经元的输出。常见的激活函数有：

- **阶跃函数**：$f(x) = \begin{cases} 1, & x \ge 0 \\ 0, & x < 0 \end{cases}$；
- **Sigmoid 函数**：
$$
f(x) = \frac{1}{1 + e^{-x}}
$$
这一函数连续、单调，且导数是其本身的函数，满足 $f'(x) = f(x)(1 - f(x))$；但是 Sigmoid 函数的导数显然 $<1$，这回导致多次求导后导数过小，从而导致梯度消失的问题；
<div style="display: flex; align-items: center; justify-content: center; flex-direction: column; padding-top:10px">
    <img src='/image/sigmoid.png' alt="" style="width:50%;"></img>
    <p style="font-size: 12px; color: gray;">Sigmoid 函数</p>
</div>

- **$\text{ReLU}$ 函数**：$f(x) = \max(0, x)$；是目前最常用的激活函数，它的导数是一个阶跃函数，这样可以避免梯度消失的问题。
- **Leaky ReLU 函数**：$f(x) = \begin{cases} x, & x \ge 0 \\ ax, & x < 0 \end{cases}$，其中 $a$ 是一个比较小的正数；是 $\text{ReLU}$ 函数的改进版，在小于零时为负，可以避免 $\text{ReLU}$ 函数中的神经元失活问题。

::: tip 提示

$\text{ReLU}$ 函数是 Rectified Linear Unit 的缩写，意为“线性整流器”。

:::

## 感知机

我们之前介绍过[感知机](./线性模型.md#感知机)，它类似一个 MP 神经元。可以使用感知机的监督学习机制作为例子，来介绍神经网络的学习机制。我们先来介绍感知机的监督学习的训练机制：

::: tip 有监督的学习机制

1. **初始化参数**：初始化权重向量 $w $和偏置 $b$，通常设为零或小的随机数。

2. **循环遍历样本**：对于每一个训练样本 $(x_i, y_i)$，计算预测值 $\hat{y} = f(w \cdot x_i + b)$。

3. **判断分类是否正确**：如果预测结果 $\hat{y} = y_i$，则样本分类正确，不更新参数；否则更新权重和偏置。

4. **参数更新**：对于分类错误的样本 $(x_i, y_i)$，按照以下规则更新参数：

   $$
   \begin{aligned}
   w &= w + \eta y_i x_i\\
   b &= b + \eta y_i
   \end{aligned}
   $$

   其中，$\eta$是学习率，用于控制参数更新的步长。通过这种更新机制，使得模型对误分类的样本更接近正确的分类边界。

5. **重复以上过程**，直到所有样本都被正确分类或达到设定的迭代次数上限。

::: 

虽然感知机可以用于一类分类问题，但是我们发现它无法解决非线性不可分的问题，因为它是单层的，也就是线性的。例如，我们**无法用感知机来解决异或问题**。

## 多层前馈神经网络

为了解决感知机无法解决非线性不可分问题的问题，我们引入了**多层感知机**（MLP）。想象一下，如果将每一层的输出作为下一层的输入，那么我们就可以构建一个多层感知机网络。由于我们的**激活函数是非线性的**，所以整个网络也是非线性的，这就可以解决一些非线性问题。

一般来说，多层感知机至少包含三层：输入层、隐藏层和输出层。其中，隐藏层可以有多层。这就是一个**多层网络**。此外，神经元之间不存在同层连接和跨层连接，这就叫做**前馈神经网络**。一般来说，输入层的神经元个数是与输入数据的维度相同，输出层的神经元个数是与输出数据的维度相同，而隐藏层的神经元个数是可以自由设定的。

多层前馈神经网络的工作原理是：通过**前向传播**，将输入信号从输入层传递到输出层，计算出输出值；然后通过**反向传播**，根据输出值和真实值的误差，**逐层反向**调整神经元的参数，使得误差最小化。可以看出，反向传播借鉴了感知机的监督学习机制。

多层前馈神经网络具有强大的表示能力。已经证明，**仅需一个**包含**足够多神经元**的隐藏层，多层前馈神经网络就可以**以任意精度逼近任何连续函数** $\text{[Hornik et al., 1989]}$。

### 反向传播算法

反向传播算法是迄今为止最常用且最成功的神经网络算法，可以用于多种任务。它是一种**基于梯度下降**的优化算法，用于调整神经网络的参数，使得网络的输出尽可能接近真实值。

我们给定训练集 $D = \{(\mathbf x_1, \mathbf y_1), (\mathbf x_2, \mathbf y_2), \cdots, (\mathbf x_n, \mathbf y_n)\}$，其中 $\mathbf x_i$ 是一个 $d$ 维输入，$\mathbf y_i$ 是 $l$ 维的真实输出。我们的目标是最小化损失函数 $L(\mathbf y, \hat{\mathbf y})$，其中 $\hat{\mathbf y}$ 是网络的输出。假设只有一个隐藏层，隐藏层的神经元个数为 $q$。可以算出，需要通过学习的参数数量为 $(d+l+1)q + l$。

<div style="display: flex; align-items: center; justify-content: center; flex-direction: column; padding-top:10px">
    <img src='/image/Screen Shot 2024-11-07 at 9.08.38 PM.png' alt="" style="width:70%;"></img>
    <p style="font-size: 12px; color: gray;">神经网络的示例</p>
</div>

对于一个样本 $(\mathbf x_k, \mathbf y_k)$，假定网络的实际输出为 $\hat{\mathbf y}_k = (\hat{y}_1^k, \hat{y}_2^k, \cdots)$，那么整个网络在这个样本的均方误差为

$$
E_k = \frac{1}{2} \sum_{j=1}^l (\hat{y}_{j}^k - y_{j}^k)^2
$$

对于一个隐藏层参数 $w_{hj}$ 而言，根据误差 $E_k$，我们求出 $w_{hj}$ 的**修正方向**为 $\frac{\partial E_k}{\partial w_{hj}}$ 的反方向（注意，这里是[向量求导](./数学基础补充.md#矩阵的导数)）。我们可以通过**链式法则**求出

$$
\frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat{y}_{j}^k} \cdot \frac{\partial \hat{y}_{j}^k}{\partial \beta_{j}^k} \cdot \frac{\partial \beta_{j}^k}{\partial w_{hj}}
$$

其中，$\beta_{j}^k$ 是输出层的输入。这一链式法则成立的原因是：$w_{hj}$ 影响了 $\beta_{j}^k$，而 $\beta_{j}^k$ 又影响了 $\hat{y}_{j}^k$，最终影响了 $E_k$。因此，我们将修正方向乘以一个**学习率** $\eta$，就可以得到 $w_{hj}$ 的修正量为

$$
\Delta w_{hj} = -\eta \frac{\partial E_k}{\partial w_{hj}}
$$

类似地，我们可以下面的公式求出前一层的参数 $v_{ih}$ 的修正量

$$
\Delta v_{ih} = -\eta \frac{\partial E_k}{\partial v_{ih}} 
$$

一般来说，前层参数的修正量需要根据后层参数的修正量来计算，这就是**反向传播**算法的基本思想。

:::details 一个使用 Sigmoid 函数的例子

:::

此时，对于这个样本 $(\mathbf x_k, \mathbf y_k)$，我们可以得到一些权重的更新信息。此时出现了两种思路：一种是每次针对单个训练样例来更新权重，这种方法是**标准 BP 算法**；另一种是每次针对整个训练集来更新权重，这种方法是**累计 BP 算法**。当然，还有一种折中的方法，即**小批量更新**（*Mini Batch*）。

:::warning 关于学习率

一般来说，学习率 $\eta$ 是一个超参数，需要根据实际情况来调整。如果学习率过大，可能会导致网络震荡；如果学习率过小，可能会导致网络收敛缓慢。

:::