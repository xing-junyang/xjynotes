# 神经网络

神经网络是利用大量节点（神经元）之间的连接关系来进行计算的模型。神经网络的基本结构是由多个神经元组成的网络，每个神经元接收来自其他神经元的输入信号，并产生输出信号。神经网络的学习过程是通过调整神经元之间的连接权重来实现的。

::: tip 仿生学

<div style="display: flex; flex-direction: row; justify-content: space-between; width: 99%; margin: 2% 0.5%">
    <p style="width: 65%">
神经网络是一种仿生学的模型，它模拟了生物神经元之间的连接方式。在生物神经元中，每个神经元都有一个细胞体，细胞体上有许多树突，树突接收来自其他神经元的信号，然后将信号传递给细胞体。细胞体会根据接收到的信号的强度来决定是否激活，如果激活了，细胞体就会向轴突发送信号。轴突是一个长的细胞突，它会将信号传递给其他神经元的树突。
    </p>
    <img src="/image/Blausen_0657_MultipolarNeuron.png" style="width: 33%;" alt="Neuron">
</div>

可以带着这个问题思考：**神经网络和生物神经元的相似之处在哪里？**

::: 

## MP 神经元的基本结构

对于每个神经元，我们可以用一个 MP 神经元来模拟。MP 神经元的基本结构如下：

- **输入** $X = [x_1, x_2, x_3, \cdots]$；
- **权重** $W = [w_1, w_2, w_3, \cdots]$；
- **激活函数** $f(net)$，其中 $net = \sum_{i=1}^n w_i x_i + b$；
- **偏置单元** $b$，也称为阈值。

我们可以利用简单的 MP 神经元来实现逻辑运算（使用**阶跃函数**作为激活函数）：

- **与门**：令 $w_1 = 1, w_2 = 1, b = -1.5$；
- **或门**：令 $w_1 = 1, w_2 = 1, b = -1$；
- **非门**：令 $w_1 = -1, b = 0.5$。

### 激活函数

激活函数决定了单个神经元的输出。常见的激活函数有：

- **阶跃函数**：$f(x) = \begin{cases} 1, & x \ge 0 \\ 0, & x < 0 \end{cases}$；
- **Sigmoid 函数**：
$$
f(x) = \frac{1}{1 + e^{-x}}
$$
这一函数连续、单调，且导数是其本身的函数，满足 $f'(x) = f(x)(1 - f(x))$；但是 Sigmoid 函数的导数显然 $<1$，这回导致多次求导后导数过小，从而导致梯度消失的问题；
<div style="display: flex; align-items: center; justify-content: center; flex-direction: column; padding-top:10px">
    <img src='/image/sigmoid.png' alt="" style="width:50%;"></img>
    <p style="font-size: 12px; color: gray;">Sigmoid 函数</p>
</div>

- **$\text{ReLU}$ 函数**：$f(x) = \max(0, x)$；是目前最常用的激活函数，它的导数是一个阶跃函数，这样可以避免梯度消失的问题。
- **Leaky ReLU 函数**：$f(x) = \begin{cases} x, & x \ge 0 \\ ax, & x < 0 \end{cases}$，其中 $a$ 是一个比较小的正数；是 $\text{ReLU}$ 函数的改进版，在小于零时为负，可以避免 $\text{ReLU}$ 函数中的神经元失活问题。

::: tip 提示

$\text{ReLU}$ 函数是 Rectified Linear Unit 的缩写，意为“线性整流器”。

:::

## 感知机

我们之前介绍过[感知机](./线性模型.md#感知机)，它类似一个 MP 神经元。可以使用感知机的监督学习机制作为例子，来介绍神经网络的学习机制。我们先来介绍感知机的监督学习的训练机制：

::: tip 有监督的学习机制

1. **初始化参数**：初始化权重向量 $w $和偏置 $b$，通常设为零或小的随机数。

2. **循环遍历样本**：对于每一个训练样本 $(x_i, y_i)$，计算预测值 $\hat{y} = f(w \cdot x_i + b)$。

3. **判断分类是否正确**：如果预测结果 $\hat{y} = y_i$，则样本分类正确，不更新参数；否则更新权重和偏置。

4. **参数更新**：对于分类错误的样本 $(x_i, y_i)$，按照以下规则更新参数：

   $$
   \begin{aligned}
   w &= w + \eta y_i x_i\\
   b &= b + \eta y_i
   \end{aligned}
   $$

   其中，$\eta$是学习率，用于控制参数更新的步长。通过这种更新机制，使得模型对误分类的样本更接近正确的分类边界。

5. **重复以上过程**，直到所有样本都被正确分类或达到设定的迭代次数上限。

::: 

虽然感知机可以用于一类分类问题，但是我们发现它无法解决非线性不可分的问题，因为它是单层的，也就是线性的。例如，我们**无法用感知机来解决异或问题**。

## 多层前馈神经网络

为了解决感知机无法解决非线性不可分问题的问题，我们引入了**多层感知机**（MLP）。想象一下，如果将每一层的输出作为下一层的输入，那么我们就可以构建一个多层感知机网络。由于我们的**激活函数是非线性的**，所以整个网络也是非线性的，这就可以解决一些非线性问题。

一般来说，多层感知机至少包含三层：输入层、隐藏层和输出层。其中，隐藏层可以有多层。这就是一个**多层网络**。此外，神经元之间不存在同层连接和跨层连接，这就叫做**前馈神经网络**。一般来说，输入层的神经元个数是与输入数据的维度相同，输出层的神经元个数是与输出数据的维度相同，而隐藏层的神经元个数是可以自由设定的。

多层前馈神经网络的工作原理是：通过**前向传播**，将输入信号从输入层传递到输出层，计算出输出值；然后通过**反向传播**，根据输出值和真实值的误差，**逐层反向**调整神经元的参数，使得误差最小化。可以看出，反向传播借鉴了感知机的监督学习机制。

多层前馈神经网络具有强大的表示能力。已经证明，**仅需一个**包含**足够多神经元**的隐藏层，多层前馈神经网络就可以**以任意精度逼近任何连续函数** $\text{[Hornik et al., 1989]}$。

### 反向传播算法

反向传播算法是迄今为止最常用且最成功的神经网络算法，可以用于多种任务。它是一种**基于梯度下降**的优化算法，用于调整神经网络的参数，使得网络的输出尽可能接近真实值。

我们给定训练集 $D = \{(\mathbf x_1, \mathbf y_1), (\mathbf x_2, \mathbf y_2), \cdots, (\mathbf x_n, \mathbf y_n)\}$，其中 $\mathbf x_i$ 是一个 $d$ 维输入，$\mathbf y_i$ 是 $l$ 维的真实输出。我们的目标是最小化损失函数 $L(\mathbf y, \hat{\mathbf y})$，其中 $\hat{\mathbf y}$ 是网络的输出。假设只有一个隐藏层，隐藏层的神经元个数为 $q$。可以算出，需要通过学习的参数数量为 $(d+l+1)q + l$。

<div style="display: flex; align-items: center; justify-content: center; flex-direction: column; padding-top:10px">
    <img src='/image/Screen Shot 2024-11-07 at 9.08.38 PM.png' alt="" style="width:70%;"></img>
    <p style="font-size: 12px; color: gray;">神经网络的示例</p>
</div>

对于一个样本 $(\mathbf x_k, \mathbf y_k)$，假定网络的实际输出为 $\hat{\mathbf y}_k = (\hat{y}_1^{(k)}, \hat{y}_2^{(k)}, \cdots)$，那么整个网络在这个样本的均方误差为

$$
E_k = \frac{1}{2} \sum_{j=1}^l (\hat{y}_{j}^{(k)} - y_{j}^{(k)})^2
$$

我们首先来解释一下下面使用的各个参数的含义：
- $\hat{y}_{j}^{(k)} = f(\beta_j^{(k)}-\theta_j^{(k)})$ 是网络在 $k$ 样本下的第 $j$ 个输出神经元的**输出**；
- $\theta_j^{(k)}$ 是输出层第 $j$ 个输出神经元的**阈值**；
- $\beta_j^{(k)} = \sum_{h=1}^{q} w_{hj}^{(k)}b_h^{(k)}$ 是输出层的输入和权重的**线性组合结果**；
- $w_{hj}^{(k)}$ 是**隐藏层到输出层的权重**；
- $b_h^{(k)} = f(a_h^{(k)}-\gamma_h^{(k)})$ 是隐藏层的输出，也是**输出层的输入**；
- $\gamma_h^{(k)}$ 是隐藏层第 $h$ 个神经元的**阈值**；
- $a_h^{(k)} = \sum_{i=1}^{d} v_{ih}x_i^{(k)}$ 是隐藏层的输入和权重的**线性组合结果**；
- $v_{ih}^{(k)}$ 是**输入层到隐藏层的权重**；
- $x_i^{(k)}$ 是输入层的输入；


对于一个隐藏层参数 $w_{hj}$ 而言，根据误差 $E_k$，我们求出 $w_{hj}$ 的**修正方向**为 $\frac{\partial E_k}{\partial w_{hj}}$ 的反方向（注意，这里是[向量求导](./数学基础补充.md#矩阵的导数)）。我们可以通过**链式法则**求出

$$
\frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat{y}_{j}^{(k)}} \cdot \frac{\partial \hat{y}_{j}^{(k)}}{\partial \beta_{j}^{(k)}} \cdot \frac{\partial \beta_{j}^{(k)}}{\partial w_{hj}^{(k)}}
$$

其中，$\beta_{j}^k$ 是输出层的输入。这一链式法则成立的原因是：$w_{hj}$ 影响了 $\beta_{j}^k$，而 $\beta_{j}^k$ 又影响了 $\hat{y}_{j}^k$，最终影响了 $E_k$。因此，我们将修正方向乘以一个**学习率** $\eta$，就可以得到 $w_{hj}$ 的修正量为

$$
\Delta w_{hj} = -\eta \frac{\partial E_k}{\partial w_{hj}^{(k)}}
$$

类似地，我们可以下面的公式求出前一层的参数 $v_{ih}$ 的修正量

$$
\Delta v_{ih} = -\eta \frac{\partial E_k}{\partial v_{ih}^{(k)}} 
$$

一般来说，前层参数的修正量需要根据后层参数的修正量来计算，这就是**反向传播**算法的基本思想。

:::details 一个使用 Sigmoid 函数的例子

Sigmoid 函数的导数是 $f'(x) = f(x)(1 - f(x))$，因此，我们可以得到

$$
\begin{aligned}
\frac{\partial E_k}{\partial \hat{y}_{j}^{(k)}}\cdot \frac{\partial \hat{y}_{j}^{(k)}}{\partial \beta_{j}^{(k)}} &= -(\hat{y}_{j}^{(k)} - y_{j}^{(k)}) \cdot f'(\beta_{j}^{(k)}-\theta_{j}^{(k)})\\
&= -(\hat{y}_{j}^{(k)} - y_{j}^{(k)}) \cdot \hat{y}_{j}^{(k)} \cdot (1 - \hat{y}_{j}^{(k)})\\
&= g_j^{(k)}
\end{aligned}
$$

而根据 $\beta_{j}^{(k)} = \sum_{h=1}^q w_{hj} \cdot b_h^{(k)}$，我们可以得到

$$
\frac{\partial \beta_{j}^{(k)}}{\partial w_{hj}^{(k)}} = b_h^{(k)}
$$

所以，我们有 $\Delta w_{hj}^{(k)} = \eta g_j^{(k)} b_h^{(k)}$。类似地，我们可以计算出来 $\Delta v_{ih}^{(k)} = \eta e_h^{(k)} x_i^{(k)}$，其中有

$$
\begin{aligned}
e_h^{(k)} &= b_h^{(k)}(1 - b_h^{(k)}) \sum_{j=1}^l g_j^{(k)} w_{hj}^{(k)}
\end{aligned}
$$

可以看出，**前层的权重更新信息需要后层的权重更新信息来计算**，这也就是反向传播算法的基本依据。
:::

此时，对于这个样本 $(\mathbf x_k, \mathbf y_k)$，我们可以得到一些权重的更新信息。此时出现了两种思路：一种是每次针对单个训练样例来更新权重，这种方法是**标准 BP 算法**；另一种是每次针对整个训练集来更新权重，这种方法是**累计 BP 算法**。当然，还有一种折中的方法，即**小批量更新**（*Mini Batch*）。

:::warning 关于学习率

一般来说，学习率 $\eta$ 是一个超参数，需要根据实际情况来调整。如果学习率过大，可能会导致网络震荡；如果学习率过小，可能会导致网络收敛缓慢。

:::

### 梯度下降

前文我们了学习率这一概念，它其实就是梯度下降算法中的**步长**。我们知道，梯度的方向是函数值增加最快的方向，而梯度的反方向就是函数值减小最快的方向。因此，我们可以通过**梯度的反方向**来调整参数，使得函数值减小。

对于一个函数 $f(\mathbf x)$，利用梯度下降来寻找最小值的过程如下：

1. **初始化参数**：初始化参数 $\mathbf x$；
2. **计算梯度**：计算函数 $f(\mathbf x)$ 在 $\mathbf x$ 处的梯度 $\nabla f(\mathbf x)$；
3. **更新参数**：根据梯度的反方向，更新参数 $\mathbf x$：
   $$
   \mathbf x = \mathbf x - \eta \nabla f(\mathbf x)
   $$
4. **重复以上过程**，直到满足**停止条件**。停止条件可以是迭代次数达到上限，或者梯度的范数小于一个阈值。

下面是一个 $\mathbf x$ 是二维向量的例子，其中的星号代表了每次迭代 $\mathbf x$ 的位置：

<div style="display: flex; align-items: center; justify-content: center; flex-direction: column; padding-top:10px">
    <img src='/image/gradient_decent.webp' alt="" style="width:70%;"></img>
    <p style="font-size: 12px; color: gray;">一个梯度减小的示例</p>
</div>

可以看出，梯度下降算法没办法保证找到全局最优解，但是可以保证找到**局部最优解**。所以，我们应当在神经网络中多次随机初始化参数，然后选择最优的参数。

### 损失函数
#### 分类：交叉熵损失函数

交叉熵是信息论中的一个概念，用于衡量两个概率分布之间的**相似度**。例如，对于两个概率分布 $p$ 和 $q$，它们之间的交叉熵定义为

$$
H(p, q) = -\sum_{x} p(x) \log q(x)
$$


$$
H(p, q)  = H(p) + D_{KL}(p||q)
$$

其中，$H(p)$ 是 $p$ 的熵，$D_{KL}(p||q)$ 是 $p$ 和 $q$ 之间的 **KL 散度**名，用于衡量使用基于 $q$ 的编码来编码来自 $p$ 的样本平均所需的额外的位元数。在神经网络中，我们通常使用交叉熵损失函数来衡量网络的输出和真实值之间的差异。对于一个样本 $(\mathbf x_k, \mathbf y_k)$，我们可以定义交叉熵损失函数为

$$
L(\mathbf y_k, \hat{\mathbf y}_k) = -\sum_{j=1}^l y_{j}^{(k)} \log \hat{y}_{j}^{(k)}
$$

相当于衡量 `one-hot` 的真实标签和预测的各个标签的概率之间的差异。

#### 回归：均方误差损失函数

对于回归问题，我们通常使用经典的均方误差损失函数来衡量网络的输出和真实值之间的差异。对于一个样本 $(\mathbf x_k, \mathbf y_k)$，我们可以定义均方误差损失函数为

$$
L(\mathbf y_k, \hat{\mathbf y}_k) = \frac{1}{2} \sum_{j=1}^l (\hat{y}_{j}^{(k)} - y_{j}^{(k)})^2, \quad \text{(L2 Norm)}
$$

$$
L(\mathbf y_k, \hat{\mathbf y}_k) = \sum_{j=1}^l |\hat{y}_{j}^{(k)} - y_{j}^{(k)}|, \quad \text{(L1 Norm)}
$$

#### 三元组损失

$$
E = \sum_{a,p,n} \max(0, m + d(\mathbf x_a, \mathbf x_p) - d(\mathbf x_a, \mathbf x_n))
$$

其中，$\mathbf x_a$ 是锚点，$\mathbf x_p$ 是正样本，$\mathbf x_n$ 是负样本，$d$ 是距离函数，$m$ 是一个大于零超参数。这一损失函数的意思是，希望锚点和正样本之间的距离尽可能小，而锚点和负样本之间的距离尽可能大。$m$ 控制了必要的**间隔**。

## 神经网络的经典模型

TODO