# 机器学习简述

## 机器学习是什么？

**学习是一个蕴含特定目的的知识获取过程**。其内部表现为新知识的不断建立和修正，外部表现为性能改善。同时，学习既需要外部的材料，也需要内部的推理与记忆的过程。

（广义的机器学习）**任何通过数据训练的学习算法都属于机器学习**。

**机器学习整体上分为三个层面**：**数据层面**、**模型层面**和**学习层面**。

### 数据层面

数据的类型和特点主要有以下几点，在选择机器学习模型上要首先对数据进行考量：

- 静态和动态
- 小数据和大数据
- 同质和异质
- 单态和多态
- 带噪和纯净

### 模型层面

- 线性模型
- 非线性模型

### 学习层面

- 经典学习方法
- 现代学习方法

学习方法的关系：深度学习 $\in$ 表示学习 $\in$ 机器学习 $\in$ 人工智能

## 机器学习的理论基础

最重要的理论模型：$\mathrm{PAC}$​ **概率近似正确**

## 基本术语

- 监督学习/无监督学习
- 数据集：训练集、测试集（可能还有验证集）
- 示例、样例：指属性空间表的每一**行**
- 属性、特征：指属性空间表的每一**列**
- 属性空间、样本空间、输入空间：由各属性张成的空间
- 假设（可能的最优参数组合）、真相（实际中的）、学习器
- 分类和回归：输出是否是离散值

## 归纳偏好

机器学习算法在学习过程中对某种类型假设的偏好。偏好是必然存在的。在实际模型选择和训练中，即要考虑算法的归纳偏好，还要防止**过拟合**。

一般原则：**奥卡姆剃刀**。

另一个理论模型：$\mathrm{NFL}$ **没有免费的午餐**。一个算法若在某些问题上比另一个算法好，那就必然存在一些问题让这个算法没有那个算法好。

## 统计学基本概念

**简单统计概念**

- 众数、中位数、平均数、方差、极差等
- **协方差** $\mathrm{cov}(X,Y)=E((X-E(X))(Y-E(Y)))$
- 协方差矩阵

**距离度量函数**。如两个样本向量 $x_i, x_j \in \mathbb{R}^d$，则它们的各个函数计算方法如下

- 欧式距离 $d(x_i, x_j)=||x_i-x_j||_2=\sqrt{(x_i- x_j)^T(x_i- x_j)}$
- 余弦相似性（类似角度）$s(x_i, x_j)=\frac{x_i^Tx_j}{||x_i||\cdot||x_j||}$
- 曼哈顿距离 $d(x_i, x_j)=||x_i-x_j||_1$
- 切比雪夫距离 $d(x_i, x_j)=||x_i-x_j||_{\infty}$
- 马氏距离 $d_M(x_i, x_j)=\sqrt{(x_i- x_j)^TM(x_i- x_j)}$

**函数凸凹性**质、凸优化（可以看《凸优化》这本书入门）

**高斯分布**

**概率**

## 新型机器学习发展趋势

- 模型层面：**大模型+领域知识，大模型+多模态信息/结构信息，小模型+模型蒸馏+量化**
- 优化层面：**在线/增量学习、分布式学习+异步优化、加速现有算法**
- 数据层面：**大数据（带噪声数据学习、多模态数据学习）、小数据（数据提炼蒸馏）**

## 一些机器学习的例子

- $\mathrm{AlphaGo}~(2015)$​​
- $\mathrm{CLIP}~(2022)$​  文本+图像的多模态大模型，通过文本来索引图像
- $\mathrm{DALL\cdot E}(2021)$ 通过文本来输出图像
- $\mathrm{AlphaFold}~(2021)$ 根据氨基酸序列进行蛋白质结构预测
- $\mathrm{AlphaCode}~(2022)$ 竞赛程序代码生成
- $\mathrm{GPT-3}~(2022)$ 
- $\mathrm{ChatGPT}~(2022)$​ 
- $\mathrm{GPT-4o}~(2024)$​ 更自然的人机交互
- $\mathrm{Sora}~(2024)$ 较强的物体一致性、连续性，初步理解世界知识