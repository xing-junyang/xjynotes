# 模型评估与选择

## 统计机器学习方法的三要素

以监督学习为例，学习方法一般由**模型、策略**和**算法**组成。

**模型**： 所要学习的条件概率分布/决策函数/映射函数。

假设空间 $\mathcal{F}$：包括所有可能的条件概率分布或决策函数集合。我们有 $\mathcal{F}=\{f|Y=f_{\theta}(X),~\theta \in \mathbb{R}^n\}$，其中 $\theta$ 为参数向量，取自 $n$ 为欧式空间（即参数空间）

**策略**：如何从假设空间中选择最优的**假设**或**模型**。

损失函数 $L$：度量模型一次预测的好坏，通常为一个非负的实函数，记为 $L(Y,f_{\theta}(X))$。常见的损失函数如下：

- $0-1$ 损失函数：$L(Y,f_{\theta}(X)) = \begin{cases} 1, & Y \neq f_{\theta}(X) \\ 0, & Y = f_{\theta}(X) \end{cases}$
- 平方损失函数：$L(Y,f_{\theta}(X)) = (Y-f_{\theta}(X))^2$

期望损失函数：$R_{exp}(f_{\theta}) = \mathbb{E}[L(Y,f_{\theta}(X))]$，期望损失函数度量平均意义下预测的好坏，但一般不能直接计算。一般来说，只能计算经验风险，即在训练集上的平均损失，其定义为：$R_{emp}(f_{\theta}) = \frac{1}{N}\sum_{i=1}^N L(y_i,f_{\theta}(x_i))$。根据大数定律，显然我们有 $n\to \infty \Rightarrow R_{emp}(f_{\theta}) \rightarrow R_{exp}(f_{\theta})$。因此，当训练集足够大时，我们可以通过最小化经验风险来最小化期望风险，也就是说经验风险最小的就是最优的模型。

::: danger 过拟合

但如果训练集的规模有限，那么此时经验风险无法逼近期望风险，因此我们需要在经验风险和模型复杂度之间做出权衡。如果模型过于复杂且训练集规模有限，那么模型很可能会出现过拟合。

为了对抗这种情况，我们需要引入结构风险最小化模型，在经验风险和模型复杂度之间做出权衡。结构风险函数在经验风险的基础上增添了正则化项，以限制模型的复杂度，其通常表示为：$R_{str}(f_{\theta}) = \frac{1}{N}\sum_{i=1}^N L(y_i,f_{\theta}(x_i)) + \lambda J(f_{\theta})$，其中 $J(f_{\theta})$ 为模型的复杂度，$\lambda$ 为正则化系数（也是一个超参数）。

:::

**算法**：学习模型的具体方法。**统计学习**通常是一个**优化问题**，小部分的优化问题有解析解，大部分的优化问题需要通过**优化算法**来求解，如**梯度下降法**、**牛顿法**等。

## 模型评估

### 泛化误差

泛化误差是学习方法对**未知数据**的预测能力，是学习方法的重要性能度量。而与此相对的是**经验误差（训练误差）**，即学习方法在训练集上的误差。显然，对于我们的目标而言，泛化误差越小越好。但经验误差也并不一定越小越好，毕竟会产生过拟合。所以我们需要引入模型评估和选择的方法。

对于回归任务，我们可以将泛化误差通过“偏差-方差”分解为：

$$
E(f_{\theta};D) = \mathrm{Bias}^2(x) + \mathrm{Var}(x) + \varepsilon^2
$$

- 偏差：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习**算法本身的拟合能力**。一般来说，$\mathrm{Bias}^2(x)=(f_{\theta}(x)-y)^2$。偏差越大，说明模型越不准确。
- 方差：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了**数据扰动所造成的影响**。一般来说，$\mathrm{Var}(x)=\mathrm{E}_D=[(f_{\theta}(x;D)-f_{\theta}(x))^2]$。方差越大，说明模型越不稳定。
- 噪声：表示在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了问题本身的难度。比如，测试集可能会不太准确。

一般而言，偏差和方差是矛盾的，降低偏差会增加方差，降低方差会增加偏差。并且，当训练不足时，偏差主导泛化误差；当训练过度时，方差主导泛化误差。

### 产生测试集

模型评估考虑的主要问题就是测试集的产生方法。模型评估的方法主要有**留出法**、**交叉验证法**和**自助法**。

- 留出法：直接将数据集 $D$ 划分为两个互斥的集合，一个作为训练集 $D_{train}$，一个作为测试集 $D_{test}$。划分时要注意数据分布的一致性和随机性，并且也要注意训练集和测试集的规模问题。
- 交叉验证法：先将数据集 $D$ 划分为 $k$ 个大小相似的互斥子集，然后每次用 $k-1$ 个子集的并集作为训练集，剩下的一个子集作为测试集，这样就可以获得 $k$ 组训练/测试集，从而可以进行 $k$ 次训练和测试，最终返回 $k$ 次测试结果的均值。
- 自助法：给定包含 $N$ 个样本的数据集 $D$，我们对其进行采样产生数据集 $D'$ 作为训练集，而 $D-D'$ 作为测试集，这样就可以进行模型评估。显然 $D'$ 中有一部分样本会重复，而另一部分样本会缺失。根据大数定律，当 $N \to \infty$ 时，样本 $x_i$ 在训练集中的概率为 $1-\frac{1}{e}$，因此在自助法中，样本 $x_i$ 会有 $\frac{1}{e}$ 的概率不会出现在训练集中。

### 调参数

- 算法的参数：一般人工设定，叫做**超参数**。
- 模型的参数：一般通过模型自己学习得到。

调参的过程与模型自己学习的过程类似，也是一个优化问题：先产生若干模型，然后基于某种评估方法进行选择。这时要用**验证集**来评估模型的好坏。因此，有时还将数据集划分为**训练集**、**验证集**和**测试集**三个部分。

### 性能度量

衡量模型泛化能力的评价标准。常见的性能度量有：

- 错误率：$E(f_{\theta}) = \frac{1}{N}\sum_{i=1}^N \mathrm{II}(y_i \neq f_{\theta}(x_i))$，其中 $\mathrm{II}$ 为指示函数。
- 精度：$A(f_{\theta}) = 1 - E(f_{\theta})$。
- 查准率：$P(f_{\theta}) = \frac{TP}{TP+FP}$，其中 $TP$ 为真正例，$FP$ 为假正例。意思就是在所有预测为正例的样本中，有多少是真正例。
- 查全率：$R(f_{\theta}) = \frac{TP}{TP+FN}$，其中 $FN$ 为假反例。意思就是在所有真正例中，有多少被预测为正例，也就是找全了多少。
- $\mathrm{F}1$ 值：$F1(f_{\theta}) = \frac{2 \times P(f_{\theta}) \times R(f_{\theta})}{P(f_{\theta}) + R(f_{\theta})}$，是查准率和查全率的调和平均数。如果 $\mathrm{F}1$ 值很高，那么说明查准率和查全率都很高。
- 推广的 $\mathrm{F}1$ 值：$F_{\beta}(f_{\theta}) = \frac{(1+\beta^2) \times P(f_{\theta}) \times R(f_{\theta})}{\beta^2 \times P(f_{\theta}) + R(f_{\theta})}$，其中 $\beta$ 为权重系数。当 $\beta=1$ 时，就是 $\mathrm{F}1$ 值；当 $\beta>1$ 时，查全率有更大的权重；当 $\beta<1$ 时，查准率有更大的权重。更复杂的推广形式是 $\mathrm{P}-\mathrm{R}$ 曲线图。
- 非均等代价下的性能度量：在实际应用中，**不同的错误会带来不同的代价**，因此我们需要引入代价敏感性度量。

### 比较检验

统计假设检验可以用来比较两个模型的性能。对于两学习器 $A$ 和 $B$，我们可以通过**交叉验证法**来比较它们的性能。假设 $A$ 的性能优于 $B$，那么我们可以通过 **McNemar 检验**来检验这个假设。McNemar 检验的原理是：对于两个模型 $A$ 和 $B$，我们可以通过它们在测试集上的性能来构建一个二元列联表，然后通过卡方检验来检验两个模型的性能是否有显著差异。