# $k$ 近邻

$k$ 近邻是一种简单的分类方法，它的基本思想是：如果一个样本在特征空间中的$k$个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别，则该样本也属于这个类别。

## $k$ 近邻分类器

该算法的主要流程如下：

- 计算测试样本 $\bar{x}$ 与训练集中每个样本 $x_i$ 的距离 $d(\bar{x},x_i)$，这里的距离计算函数 $d(x,y)$ 是距离度量函数的一种，可以参考[此篇文章](/机器学习/数学基础补充.md#距离度量函数)。
- 对于所有距离值进行排序，找到距离最近的 $k$ 个样本。
- 统计这 $k$ 个样本中每个类别的个数，选择个数最多的类别分配给 $\bar{x}$。

在这一算法中，我们一般将 $k$ 取奇数，防止平局。当 $k$ 较小时，对噪声敏感，整体模型变得复杂，容易过拟合；当 $k$ 较大时，对噪声不敏感，模型变得简单，容易欠拟合。

特别地，当 $k=1$ 时，该算法称为**最近邻算法**。

::: tip $k$ 近邻分类器的错误率

对于最近邻分类器，如果测试样本为 $\mathbf{x}$，其最近邻样本为 $\mathbf{z}$，则有：

$$
P(err) = 1 - \Sigma_{c\in \mathcal{Y}} P(c| \mathbf{x})P(c\mid \mathbf{z})

$$

此外，虽然看起来简单，但是如果假设样本独立同分布，那么最近邻分类器的泛化错误率居然不超过贝叶斯最优分类器的错误率的两倍！

:::

## $k$ 近邻回归

除了分类问题，$k$ 近邻还可以用于回归问题。在回归问题中，我们需要预测一个连续值，而不是一个类别。

在 $k$ 近邻回归中，我们可以将 $k$ 个最近邻样本的标签值的平均值作为预测值。即：

$$
\hat{y} = \frac{1}{k} \Sigma_{x_i \in N_{k}(x_0)} y_i
$$

其中 $N_{k}(x_0)$ 表示与 $x_0$ 最近的 $k$ 个样本，而 $x_0$ 是我们需要预测的样本。在我们的实际应用中，我们发现生成的回归曲线并不平滑，而具有很多尖峰。为了解决这个问题，我们可以使用核函数对回归曲线进行平滑处理。

在 $k$ 近邻回归中，我们可以使用不同的核函数，如高斯核函数、多项式核函数等。这些核函数的选择取决于我们的数据集。常用的核函数有：

- 高斯核函数：$K(x_i,x_j) = \exp(-\frac{\|x_i-x_j\|^2}{2\sigma^2})$


## 懒惰学习

$k-NN$ 算法是一种典型的懒惰学习（lazy learning）算法，也称为基于实例的学习（instance-based learning）算法。懒惰学习是指在训练阶段不需要训练模型，只需要存储训练数据，当需要预测时，才进行计算。对于 $k-NN$ 算法，它的优点是**精度很高、对异常值不敏感、并且没有数据输入的假定**。但是，它的缺点是**计算量大、存储空间大、预测速度慢**等等。

## 降低近邻计算

在实际应用中，我们可以通过一些预处理方法来降低近邻计算的复杂度。其中一种常用的方法就是**维诺图**。维诺图在低维的情况下表现较好。

TODO: 维诺图的介绍

此外，我们还可以使用**KD树**来降低近邻计算的复杂度。KD树是一种对 $k$ 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。KD树是二叉树，表示对 $k$ 维空间的一个划分。构造 KD 树相当于不断地用垂直于坐标轴的超平面将 $k$ 维空间切分，构成一系列的 $k$ 维超矩形区域。

TODO: KD 树的介绍

另一种方法是对数据进行降维。它的核心思想是通过某种数学变换将原式高位属性空间转变为低维子空间，来化解维数灾难问题。主要的方法有：

- 多维缩放算法
- 主成分分析
- Isomap 算法等

TODO：近似最近邻

哈希的核心思想是利用哈希函数把任意长度的输入映射为固定长度的输出。

## 扩展阅读

$k-NN$ 算法的缺点是它并不建立在任何概率框架上，所以无法得到关于类别的后验概率。我们可以通过定义似然函数来构造关于概率的 $k-NN$ 算法。