# 支持向量机
## 感知机

我们先来考虑一个感知机的问题。如何寻找一个能将数据集分成两类的超平面？对于线性可分的数据集，可能有很多很多选择。那么，我们应该如何选择一个“最好的”、泛化能力最强的超平面呢？我们可能会想到前文的[线性判别分析](./线性模型.md#线性判别分析linear-discriminant-analysis-lda)，但这一算法不能将数据集完全分开。直觉告诉我们，我们应该选择一个在两类样本**正中间**的超平面。这个超平面距离两类样本的最近点的距离最远。的确，这一算法在实际的模型中表现较好，并且如果利用**核函数**，该算法还适用于线性不可分的情况。有了这一直观的感受的基础，我们就给出下面的定义：

一个点对应的**间隔**是指其到分界超平面的**垂直距离**。SVM 最大化**所有训练样本**的**最小间隔**。具有最小间隔的点叫做**支持向量**。

对于一个分类超平面 $w^Tx+b=0$，其法向量为 $w$，那么点 $x$ 到超平面的距离为：

$$
\frac{|w^Tx+b|}{\|w\|}
$$

::: details 推导过程

下面的推导，可以通过平面情况画图辅助理解。

假设超平面 $f(\mathbf{x}) = \mathbf w^T\mathbf{x} + b = 0$，其中 $w$ 是超平面的法向量，$b$ 是超平面的截距。令 $\mathbf{x}$ 的投影点为 $\mathbf{x}_{\perp}$，则 $\mathbf{x} - \mathbf{x}_{\perp}$ 即为距离向量，设其大小为 $r$（可正可负），方向为 $\frac{\mathbf{w}}{\|\mathbf{w}\|}$，则有：

$$
\begin{aligned}
\mathbf{x} - \mathbf{x}_{\perp} = r\frac{\mathbf{w}}{\|\mathbf{w}\|}\\
\mathbf{x} = \mathbf{x}_{\perp} + r\frac{\mathbf{w}}{\|\mathbf{w}\|}\\
\mathbf{w}^T\mathbf{x} + b = \mathbf w^T\mathbf x_{\perp} + b+ r\frac{\mathbf{w}^T\mathbf w}{\|\mathbf{w}\|}\\
f(\mathbf x) = f(\mathbf x_{\perp}) + r\|\mathbf{w}\|
\end{aligned}
$$

由于 $f(\mathbf x_{\perp}) = 0$，所以有：

$$
r = \frac{f(\mathbf x)}{\|\mathbf{w}\|} = \frac{\mathbf w^T\mathbf x + b}{\|\mathbf{w}\|}
$$

为了方便，我们规定距离为正，那么 $|r| = \frac{|\mathbf w^T\mathbf x + b|}{\|\mathbf{w}\|}$。$\Box$

:::

## 线性支持向量机

### 分类与评价

- 怎样分类？当 $f(\mathbf{x}) >0$ 时 $\mathbf{x}$ 属于正类，当 $f(\mathbf{x}) <0$ 时 $\mathbf{x}$ 属于负类。
- 对于任何一个样例，我们可以通过将真实标签乘上预测标签，得到一个值。如果该值为正，则说明预测正确，否则预测错误。假设我们的标签满足 $y\in \{-1,1\}$，那么针对一个预测及其准确的模型，它的预测结果应该满足 $y_if(x_i) = |f(x_i)|$。

### SVM 的形式化描述

SVM 解决的问题就是求一个优化问题：
$$
\begin{aligned}
\mathrm{argmax}_{w,b} \left [\min_i \left(\frac{|\mathbf w^T \mathbf{x}_i+b|}{||\mathbf w||}\right)\right]\\
= \mathrm{argmax}_{w,b} \left [\min_i \left(\frac{y_i(\mathbf w^T \mathbf{x}_i+b)}{||\mathbf w||}\right)\right]
\end{aligned}
$$

目前为止，由于 $\mathbf w$ 没有限制，所以这一问题比较难优化。但我们发现，$\mathbf w^T x+b$ 的大小对分类的结果没有影响，因此，我们只需要关心 $(\mathbf w, b)$ 的**方向**即可。我们可以把 $\mathbf w$ 的长度限制为 1，这样我们就可以得到一个优化问题：

$$
\begin{aligned}
\mathrm{argmax}_{w,b} \left [\min_i \left(y_if(\mathbf x_i)\right)\right]\\
\text{s.t. }y_if(\mathbf x_i)>0,\quad 1\leq i\leq n\\
\mathbf w^T \mathbf w = 1
\end{aligned}
$$

假设某个最优解为 $(\mathbf w^*, b^*)$，那么我们令一个常数 $c = \min_{1\le i\le n} y_i(({\mathbf w}^{*})^T\mathbf x + b^*)$，显然，$\frac{1}{c}(w^*, b^*)$ 也是一个最优解。我们代入后发现

$$
\min_{1\le i\le n} y_i(({\frac{1}{c}\mathbf w}^{*})^T\mathbf x + \frac{b^*}{c}) = 1>0
$$

因此，我们可以限制 $\min_i y_i(\mathbf w^T \mathbf x + b) = 1$，这样我们就可以得到一个更简单的优化问题，并且不改变最优解。下面问题就变为在限制 $\min_i y_i(\mathbf w^T \mathbf x + b) = 1$ 的情况下，最大化 $\frac{1}{||\mathbf w||}$。形式化表述为

$$
\begin{aligned}
\mathrm{argmin}_{w,b}& \frac{1}{2}||\mathbf w||^2\\
\text{s.t. }y_i(\mathbf w^T \mathbf x + b) \geq 1,&\quad 1\leq i\leq n
\end{aligned}
$$

下面，我们使用拉格朗日乘子法来求解。令拉格朗日乘子为 $\alpha_i \ge 0$，则拉格朗日函数为

$$
L(\mathbf w, b, \alpha) = \frac{1}{2}||\mathbf w||^2 - \sum_{i=1}^n \alpha_i(y_i(\mathbf w^T \mathbf x + b) - 1)
$$

::: tip 为什么拉格朗日乘子非负？KKT 条件是什么？

常见的拉格朗日乘子法是在约束为等式 $g(\mathbf x) = 0$ 的情况下，求解 $E(\mathbf x)$ 的最大值或最小值。在约束为等式的这种情况下，我们不对拉格朗日乘子 $\lambda$ 的正负进行限制，只需满足 $\nabla E = \lambda \nabla g$ 即可。

下面我们考虑约束为不等式的情况。例如求解最小化问题

$$
\begin{aligned}
\mathrm{argmin}_{\mathbf x}& E(\mathbf x)\\
\text{s.t. }g(\mathbf x) \geq 0
\end{aligned}
$$

我们可以通过引入拉格朗日乘子 $\lambda$，得到拉格朗日函数

$$
L(\mathbf x, \lambda) = E(\mathbf x) - \lambda g(\mathbf x)
$$

我们尝试把它转换为求解函数 $L$ 的最小值的问题。对其求导可得

$$
\begin{aligned}
\nabla L_\mathbf x &= \nabla E - \lambda \nabla g\\
\nabla L_\lambda &= g(\mathbf x)
\end{aligned}
$$

但显然，我们需要对其施加更多的约束。首先，我们要将 $g(x)\ge 0$ 的限制继承下来。其次我们发现，如果最小值点在 $g(x) < 0$ 的范围内，则有 $\nabla E = 0$，此时 $\lambda$ 可以取 $0$ 即可；如果最小值点刚好在 $g(x) = 0$ 这一边界上，则必须有 $\nabla E$ 和 $\nabla g$ 同向（如果反向，我们沿着 $\nabla E$ 方向移动，可以使得 $E$ 减小并使得 $g$ 不变大，这样 $E$ 就不是最小值了）。所以我们必须规定 $\lambda \ge 0$。经过这一讨论，我们还发现，当取得最小值时，我们有 $\lambda g = 0$。

综合上述讨论，我们得到了 KKT 条件：

$$
\begin{aligned}
g(\mathbf x) &\ge 0\\
\lambda &\ge 0\\
\lambda g(\mathbf x) &= 0
\end{aligned}
$$

下图是另一种，$E$ 需要最大化的情况，可以进行对比思考。

<div style="display: flex; align-items: center; justify-content: center; flex-direction: column; padding-top:10px">
    <img src='/image/Screen Shot 2024-11-01 at 9.28.13 PM.png' alt="" style="width:50%;"></img>
    <p style="font-size: 12px; color: gray;">最大化 E</p>
</div>

:::

当最优时，有

$$
\begin{aligned}
\frac{\partial L}{\partial \mathbf w} &= \mathbf w - \sum_{i=1}^n \alpha_i y_i \mathbf x_i = 0\\
\frac{\partial L}{\partial b} &= -\sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$

将结果代回拉格朗日函数，我们可以得到新的拉格朗日函数 $L^{*}(\alpha)$

$$
L^{*}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j \mathbf x_i^T \mathbf x_j
$$

将我们的结果总结一下，利用 $\text{Karush-Kuhn-Tucker}$（KKT）条件，我们就得到了：

$$
\begin{aligned}
\mathbf w = \sum_{i=1}^n \alpha_i y_i \mathbf x_i\\
\sum_{i=1}^n \alpha_i y_i = 0\\
\alpha_i \ge 0\\
\alpha_i(y_i(\mathbf w^T \mathbf x + b) - 1) = 0\\
y_i(\mathbf w^T \mathbf x + b) - 1 \ge 0
\end{aligned}
$$

其中 $\alpha_i(y_i(\mathbf w^T \mathbf x + b) - 1) = 0$ 称为**互补松弛性质**。一般来说，KKT 条件不是充分必要的。

#### SVM 的对偶形式

在原来的空间中，我们研究的变量是 $\mathbf x_i$，这称为 SVM 的原始形式。而当我们经过拉格朗日乘子法后，我们研究的变量变为 $\alpha_i$，这称为 SVM 的对偶形式。我们可以**通过求解对偶问题来得到原始问题的解**。

假设我们在对偶空间中，我们已经求得了最优的 $\alpha^*$。对于 $\mathbf w^*$，我们可以通过

$$
\mathbf w^* = \sum_{i=1}^n \alpha_i^* y_i \mathbf x_i
$$

求得。对于 $b^*$，我们可以通过互补松弛性质，如果 $\alpha_i^* > 0$，那么

$$
y_i(\mathbf w^T \mathbf x_i + b) = 1
$$

由此，对于某一个特定的 $b^*_i$，我们有

$$
b^*_i = y_i - (\mathbf w^*)^T \mathbf x_i
$$

而对于所有样本，我们取其平均值即可

$$
b^* = \frac{1}{n}\sum_{\alpha_i^*>0} (y_i - (\mathbf w^*)^T \mathbf x_i)
$$

### Soft Margin 软间隔

对于原先的强约束 $y_i(\mathbf w^T \mathbf x + b) \geq 1$，我们可以引入一个松弛变量 $\xi_i \geq 0$，使得约束变为 $y_i(\mathbf w^T \mathbf x + b) \geq 1 - \xi_i$。这样，我们的模型就会对于一些线性难以区分的样本有一定的容忍度，对噪声数据的适应性也会增加。

但是，犯错误是会受到**惩罚**的。我们可以通过引入一个正则化 $C$，使得我们的优化问题变为

$$
\begin{aligned}
\mathrm{argmin}_{w,b,\xi}& \frac{1}{2}||\mathbf w||^2 + C\sum_{i=1}^n \xi_i\\
\text{s.t. }y_i(\mathbf w^T \mathbf x + b) \geq 1 - \xi_i,&\quad 1\leq i\leq n\\
\xi_i \geq 0,&\quad 1\leq i\leq n
\end{aligned}
$$

通过调整 $C$ 的大小，我们可以控制模型对于噪声数据的容忍度，或说对错误的容忍度。

在对偶空间中，我们的拉格朗日函数其实**没有改变**，只是对于 $\alpha_i$ 的约束条件变为了 $0\leq \alpha_i \leq C$。