# 支持向量机
## 感知机

我们先来考虑一个感知机的问题。如何寻找一个能将数据集分成两类的超平面？对于线性可分的数据集，可能有很多很多选择。那么，我们应该如何选择一个“最好的”、泛化能力最强的超平面呢？我们可能会想到前文的[线性判别分析](./线性模型.md#线性判别分析linear-discriminant-analysis-lda)，但这一算法不能将数据集完全分开。直觉告诉我们，我们应该选择一个在两类样本**正中间**的超平面。这个超平面距离两类样本的最近点的距离最远。的确，这一算法在实际的模型中表现较好，并且如果利用**核函数**，该算法还适用于线性不可分的情况。有了这一直观的感受的基础，我们就给出下面的定义：

一个点对应的**间隔**是指其到分界超平面的**垂直距离**。SVM 最大化**所有训练样本**的**最小间隔**。具有最小间隔的点叫做**支持向量**。

对于一个分类超平面 $w^Tx+b=0$，其法向量为 $w$，那么点 $x$ 到超平面的距离为：

$$
\frac{|w^Tx+b|}{\|w\|}
$$

::: details 推导过程

下面的推导，可以通过平面情况画图辅助理解。

假设超平面 $f(\mathbf{x}) = \mathbf w^T\mathbf{x} + b = 0$，其中 $w$ 是超平面的法向量，$b$ 是超平面的截距。令 $\mathbf{x}$ 的投影点为 $\mathbf{x}_{\perp}$，则 $\mathbf{x} - \mathbf{x}_{\perp}$ 即为距离向量，设其大小为 $r$（可正可负），方向为 $\frac{\mathbf{w}}{\|\mathbf{w}\|}$，则有：

$$
\begin{aligned}
\mathbf{x} - \mathbf{x}_{\perp} = r\frac{\mathbf{w}}{\|\mathbf{w}\|}\\
\mathbf{x} = \mathbf{x}_{\perp} + r\frac{\mathbf{w}}{\|\mathbf{w}\|}\\
\mathbf{w}^T\mathbf{x} + b = \mathbf w^T\mathbf x_{\perp} + b+ r\frac{\mathbf{w}^T\mathbf w}{\|\mathbf{w}\|}\\
f(\mathbf x) = f(\mathbf x_{\perp}) + r\|\mathbf{w}\|
\end{aligned}
$$

由于 $f(\mathbf x_{\perp}) = 0$，所以有：

$$
r = \frac{f(\mathbf x)}{\|\mathbf{w}\|} = \frac{\mathbf w^T\mathbf x + b}{\|\mathbf{w}\|}
$$

为了方便，我们规定距离为正，那么 $|r| = \frac{|\mathbf w^T\mathbf x + b|}{\|\mathbf{w}\|}$。$\Box$

:::

## 线性支持向量机

### 分类与评价

- 怎样分类？当 $f(\mathbf{x}) >0$ 时 $\mathbf{x}$ 属于正类，当 $f(\mathbf{x}) <0$ 时 $\mathbf{x}$ 属于负类。
- 对于任何一个样例，我们可以通过将真实标签乘上预测标签，得到一个值。如果该值为正，则说明预测正确，否则预测错误。假设我们的标签满足 $y\in \{-1,1\}$，那么针对一个预测及其准确的模型，它的预测结果应该满足 $y_if(x_i) = |f(x_i)|$。

### SVM 的形式化描述

SVM 解决的问题就是求一个优化问题：
$$
\begin{aligned}
\mathrm{argmax}_{w,b} \left [\min_i \left(\frac{|\mathbf w^T \mathbf{x}_i+b|}{||\mathbf w||}\right)\right]\\
= \mathrm{argmax}_{w,b} \left [\min_i \left(\frac{y_i(\mathbf w^T \mathbf{x}_i+b)}{||\mathbf w||}\right)\right]
\end{aligned}
$$

目前为止，由于 $\mathbf w$ 没有限制，所以这一问题比较难优化。但我们发现，$\mathbf w^T x+b$ 的大小对分类的结果没有影响，因此，我们只需要关心 $(\mathbf w, b)$ 的**方向**即可。我们可以把 $\mathbf w$ 的长度限制为 1，这样我们就可以得到一个优化问题：

$$
\begin{aligned}
\mathrm{argmax}_{w,b} \left [\min_i \left(y_if(\mathbf x_i)\right)\right]\\
\text{s.t. }y_if(\mathbf x_i)>0,\quad 1\leq i\leq n\\
\mathbf w^T \mathbf w = 1
\end{aligned}
$$

假设某个最优解为 $(\mathbf w^*, b^*)$，那么我们令一个常数 $c = \min_{1\le i\le n} y_i(({\mathbf w}^{*})^T\mathbf x + b^*)$，显然，$\frac{1}{c}(w^*, b^*)$ 也是一个最优解。我们代入后发现

$$
\min_{1\le i\le n} y_i(({\frac{1}{c}\mathbf w}^{*})^T\mathbf x + \frac{b^*}{c}) = 1>0
$$

因此，我们可以限制 $\min_i y_i(\mathbf w^T \mathbf x + b) = 1$，这样我们就可以得到一个更简单的优化问题，并且不改变最优解。下面问题就变为在限制 $\min_i y_i(\mathbf w^T \mathbf x + b) = 1$ 的情况下，最大化 $\frac{1}{||\mathbf w||}$。形式化表述为

$$
\begin{aligned}
\mathrm{argmin}_{w,b}& \frac{1}{2}||\mathbf w||^2\\
\text{s.t. }y_i(\mathbf w^T \mathbf x + b) \geq 1,&\quad 1\leq i\leq n
\end{aligned}
$$

下面，我们使用拉格朗日乘子法来求解。令拉格朗日乘子为 $\alpha_i \ge 0$，则拉格朗日函数为

$$
L(\mathbf w, b, \alpha) = \frac{1}{2}||\mathbf w||^2 - \sum_{i=1}^n \alpha_i(y_i(\mathbf w^T \mathbf x + b) - 1)
$$

::: tip 为什么拉格朗日乘子非负？KKT 条件是什么？

常见的拉格朗日乘子法是在约束为等式 $g(\mathbf x) = 0$ 的情况下，求解 $E(\mathbf x)$ 的最大值或最小值。在约束为等式的这种情况下，我们不对拉格朗日乘子 $\lambda$ 的正负进行限制，只需满足 $\nabla E = \lambda \nabla g$ 即可。

下面我们考虑约束为不等式的情况。例如求解最小化问题

$$
\mathrm{argmin}_{\mathbf x} E(\mathbf x)\quad \text{s.t. }g(\mathbf x) \geq 0
$$

我们可以通过引入拉格朗日乘子 $\lambda$，得到拉格朗日函数

$$
L(\mathbf x, \lambda) = E(\mathbf x) - \lambda g(\mathbf x)
$$

我们尝试把它转换为求解函数 $L$ 的最小值的问题。对其求导可得

$$
\begin{aligned}
\nabla L_\mathbf x &= \nabla E - \lambda \nabla g\\
\nabla L_\lambda &= g(\mathbf x)
\end{aligned}
$$

但显然，我们需要对其施加更多的约束。首先，我们要将 $g(x)\ge 0$ 的限制继承下来。其次我们发现，如果最小值点在 $g(x) < 0$ 的范围内，则有 $\nabla E = 0$，此时 $\lambda$ 取 $0$ 即可；如果最小值点刚好在 $g(x) = 0$ 这一边界上，则必须有 $\nabla E$ 和 $\nabla g$ 同向（如果反向，我们沿着 $\nabla E$ 方向移动，可以使得 $E$ 减小并使得 $g$ 不变大，这样 $E$ 就不是最小值了）。所以我们必须规定 $\lambda \ge 0$。经过这一讨论，我们还发现，当取得最小值时，我们有 $\lambda g = 0$。

综合上述讨论，我们得到了 KKT 条件：

$$
\begin{aligned}
g(\mathbf x) &\ge 0\\
\lambda &\ge 0\\
\lambda g(\mathbf x) &= 0
\end{aligned}
$$

下图是另一种，$E$ 需要最大化的情况，可以进行对比思考。

<div style="display: flex; align-items: center; justify-content: center; flex-direction: column; padding-top:10px">
    <img src='/image/Screen Shot 2024-11-01 at 9.28.13 PM.png' alt="" style="width:50%;"></img>
    <p style="font-size: 12px; color: gray;">最大化 E</p>
</div>

:::

当最优时，有

$$
\begin{aligned}
\frac{\partial L}{\partial \mathbf w} &= \mathbf w - \sum_{i=1}^n \alpha_i y_i \mathbf x_i = 0\\
\frac{\partial L}{\partial b} &= -\sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$

将结果代回拉格朗日函数，我们可以得到新的拉格朗日函数 $L^{*}(\alpha)$

$$
L^{*}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j \mathbf x_i^T \mathbf x_j
$$

将我们的结果总结一下，利用 $\text{Karush-Kuhn-Tucker}$（KKT）条件，我们就得到了：

$$
\begin{aligned}
\mathbf w = \sum_{i=1}^n \alpha_i y_i \mathbf x_i\\
\sum_{i=1}^n \alpha_i y_i = 0\\
\alpha_i \ge 0\\
\alpha_i(y_i(\mathbf w^T \mathbf x + b) - 1) = 0\\
y_i(\mathbf w^T \mathbf x + b) - 1 \ge 0
\end{aligned}
$$

其中 $\alpha_i(y_i(\mathbf w^T \mathbf x + b) - 1) = 0$ 称为**互补松弛性质**。一般来说，KKT 条件不是充分必要的。

#### SVM 的对偶形式

在原来的空间中，我们研究的变量是 $\mathbf x_i$，这称为 SVM 的原始形式。而当我们经过拉格朗日乘子法后，我们研究的变量变为 $\alpha_i$，这称为 SVM 的对偶形式。我们可以**通过求解对偶问题来得到原始问题的解**。

假设我们在对偶空间中，我们已经求得了最优的 $\alpha^*$。对于 $\mathbf w^*$，我们可以通过

$$
\mathbf w^* = \sum_{i=1}^n \alpha_i^* y_i \mathbf x_i
$$

求得。对于 $b^*$，我们可以通过互补松弛性质，如果 $\alpha_i^* > 0$，那么

$$
y_i(\mathbf w^T \mathbf x_i + b) = 1
$$

由此，对于某一个特定的 $b^*_i$，我们有

$$
b^*_i = y_i - (\mathbf w^*)^T \mathbf x_i
$$

而对于所有样本，我们取其平均值即可

$$
b^* = \frac{1}{n}\sum_{\alpha_i^*>0} (y_i - (\mathbf w^*)^T \mathbf x_i)
$$

### Soft Margin 软间隔

对于原先的强约束 $y_i(\mathbf w^T \mathbf x + b) \geq 1$，我们可以引入一个松弛变量 $\xi_i \geq 0$，使得约束变为 $y_i(\mathbf w^T \mathbf x + b) \geq 1 - \xi_i$。这样，我们的模型就会对于一些线性难以区分的样本有一定的容忍度，对噪声数据的适应性也会增加。

但是，犯错误是会受到**惩罚**的。我们可以通过引入一个正则化 $C$，使得我们的优化问题变为

$$
\begin{aligned}
\mathrm{argmin}_{w,b,\xi}& \frac{1}{2}||\mathbf w||^2 + C\sum_{i=1}^n \xi_i\\
\text{s.t. }y_i(\mathbf w^T \mathbf x + b) \geq 1 - \xi_i,&\quad 1\leq i\leq n\\
\xi_i \geq 0,&\quad 1\leq i\leq n
\end{aligned}
$$

通过调整 $C$ 的大小，我们可以控制模型对于噪声数据的容忍度，或说对错误的容忍度。

在对偶空间中，我们的拉格朗日函数其实**没有改变**，只是对于 $\alpha_i$ 的约束条件变为了 $0\leq \alpha_i \leq C$。

## 非线性支持向量机

当数据集是线性可分的时候，线性支持向量机的表现是非常好的。但是，当数据集是线性不可分的时候，我们就需要引入**核函数**来将数据映射到高维空间中，使得数据集变得线性可分。显然，当原始空间是有限维时，那么一定存在一个高维特征空间使样本可分。这就是一种非线性支持向量机的基本思想。

### 核函数

我们可以看下面一个例子： 假设 $\mathbf x = (x_1, x_2), \mathbf y = (y_1, y_2)$，我们可以定义一个二次多项式核函数 $K(\mathbf x, \mathbf y) = (\mathbf x^T \mathbf y + 1)^2$。展开并分解可发现：

$$
\begin{aligned}
K(\mathbf x, \mathbf y) &= (\mathbf x^T \mathbf y + 1)^2\\
&= (x_1y_1 + x_2y_2 + 1)^2\\
&= x_1^2y_1^2 + x_2^2y_2^2 + 1 + 2x_1y_1 + 2x_2y_2 + 2x_1x_2y_1y_2\\
&=
\left( \begin{array}{c}
x_1^2\\
x_2^2\\
1\\
\sqrt{2}x_1\\
\sqrt{2}x_2\\
\sqrt{2}x_1x_2
\end{array} \right)^T
\left( \begin{array}{c}
y_1^2\\
y_2^2\\
1\\
\sqrt{2}y_1\\
\sqrt{2}y_2\\
\sqrt{2}y_1y_2
\end{array} \right)
\end{aligned}
$$

如果定义 $\phi(\mathbf x) = \left( \begin{array}{c}
x_1^2\\
x_2^2\\
1\\
\sqrt{2}x_1\\
\sqrt{2}x_2\\
\sqrt{2}x_1x_2
\end{array} \right)$，那么我们可以得到 $K(\mathbf x, \mathbf y) = \phi(\mathbf x)^T \phi(\mathbf y)$。这样的 $K$ 称为**核函数**，而 $\phi$ 称为**特征映射**。原来的 $\mathbf x$ 空间称为**原始空间**，而 $\phi(\mathbf x)$ 空间称为**特征空间**。

### 核支持向量机

我们利用核函数，可以将原来的优化问题转化为：

- 对偶形式：
$$
\text{argmax}_{a} \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(\mathbf x_i, \mathbf x_j)
$$

- 分类边界：$\mathbf w = \sum_{i=1}^n \alpha_i y_i \phi(\mathbf x_i)$

- 预测：

$$
\mathbf w^T \phi(\mathbf x) = \phi(\mathbf x)^T \left [\sum_{i=1}^n \alpha_i y_i \phi(\mathbf x_i)\right] = \sum_{i=1}^n \alpha_i y_i K(\mathbf x, \mathbf x_i)
$$

这样，我们就可以通过核函数将数据映射到高维空间中，使得数据集**在新的空间中变得线性可分**。

下面，我们给出判断特征映射存在的一个充要条件：

::: tip $\text{Mercer}$ 条件

对于任何满足 $\int g^2(\mathbf u) \mathrm{d} \mathbf{u} < \infty$ 的非零函数（平方可积函数），对称函数 $K$ 满足:

$$
\iint K(\mathbf x, \mathbf u)g(\mathbf x)g(\mathbf u) ~\mathrm{d} \mathbf x \mathrm{d} \mathbf u \geq 0
$$

则称 $K$ 满足 $\text{Mercer}$ 条件。

一种等价形式是：对于任何一个样本集合 $\{\mathbf x_1, \mathbf x_2, \cdots, \mathbf x_n\}$，其对应的核矩阵 $\mathbf K = K(x_i,x_j)$ 都满足 $\mathbf K$ 是半正定的，那么 $K$ 满足 $\text{Mercer}$ 条件。

::: 

### 常见核函数

- 线性核函数：$K(\mathbf x, \mathbf y) = \mathbf x^T \mathbf y$
- 多项式核函数：$K(\mathbf x, \mathbf y) = (\gamma \mathbf x^T \mathbf y + c)^d$
- 高斯核函数：$K(\mathbf x, \mathbf y) = \exp(-\gamma ||\mathbf x - \mathbf y||^2)$

<div style="display: flex; align-items: center; justify-content: center; flex-direction: column; padding-top:10px">
    <img src='/image/Screen Shot 2024-11-07 at 9.39.37 PM.png' alt="" style="width:50%;"></img>
    <p style="font-size: 12px; color: gray;">一个核的例子（蓝色：Ground Truth，绿色：学习分类边界）</p>
</div>

::: tip RBF 核函数

在机器学习中（尤其是支持向量机理论中），径向基函数核（Radial Basis Function Kernel，简称 RBF 核）特指高斯核函数
$$
K(\mathbf x, \mathbf y) = \exp(-\gamma ||\mathbf x - \mathbf y||^2)
$$

它的特征映射函数为无穷维，其形式为

$$
\phi(\mathbf x) = \exp\left( - \gamma ||\mathbf x||^2 \right)\left(1, \sqrt{\frac{2\gamma}{1!}}||\mathbf x||, \sqrt{\frac{(2\gamma)^2}{2!}}||\mathbf x||^2, \cdots, \sqrt{\frac{(2\gamma)^n}{n!}}||\mathbf x||^n, \cdots\right)
$$

这是因为

$$

\begin{aligned}
K(\mathbf x, \mathbf y) &= \exp(-\gamma ||\mathbf x - \mathbf y||^2)\\
&=\exp \left(-\gamma(||\mathbf x||^2+||\mathbf y||^2-2\mathbf x^T\mathbf y)\right)\\

&=\exp \left(-\gamma ||\mathbf x||^2\right)\exp \left(-\gamma ||\mathbf y||^2\right)\exp \left(2\gamma\mathbf x^T\mathbf y \right)\\

&=\exp \left(-\gamma ||\mathbf x||^2\right)\exp \left(-\gamma ||\mathbf y||^2\right) \left(\sum_{i = 0}^{\infty}\frac{(2\gamma\mathbf x^T\mathbf y)^i}{i!}\right)\\

&=\left [\exp \left(-\gamma ||\mathbf x||^2\right)\left(\sqrt{\frac{(2\gamma)^i}{i!}}||\mathbf x||^i\right)_{i=0}^{\infty}\right ]^T\exp \left(-\gamma ||\mathbf y||^2\right)\left(\sqrt{\frac{(2\gamma)^i}{i!}}||\mathbf y||^i\right)_{i=0}^{\infty}\\

&=\left \langle \phi(\mathbf x),\phi(\mathbf y) \right \rangle

\end{aligned}

$$

容易看出，$K$ 的值域是 $(0,1]$，且 $K(\mathbf x, \mathbf x) = 1$。RBF 核函数有一个参数 $\gamma$，它相当于控制了高斯分布的**方差**。$\gamma$ 越大，高斯分布的方差越小，模型对于训练数据的**拟合程度越高**，但是**泛化能力会降低**；反之，$\gamma$ 减小可以提高泛化能力，但是可能欠拟合。

使用 `SciKit-Learn` 中的 `SVC` 类，我们可以通过 **`kernel='rbf'`** 来使用 RBF 核函数。

```python
from sklearn.svm import SVC

rbf_svm = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)
rbf_svm.fit(X_train, y_train)
y_pred = rbf_svm.predict(X_test)
```

`SVC` 类构造函数的参数 `gamma` 控制了高斯核函数的 **$\gamma$ 参数**，参数 `C` 是前文**软间隔的惩罚系数**。

:::

下面是一个**使用 RBF 核函数的简单例子**，读者可以下载查看。<button class="button-download" onclick="window.open('/pdf/svm_example.pdf')">下载 PDF</button>

<style>
.button-download {
    background-color: #42b983; /* Green */
    border: none;
    color: white;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    font-weight: bold;
    cursor: pointer;
    border-radius: 20px;
    height: 40px;
    line-height: 40px;
    padding: 0 20px;
    margin-left: 25px;
    transition: all 0.3s ease; /* 添加transition效果 */
}

.button-download:hover {
    background-color: #00e175;
    font-weight: 800;
    transform: scale(1.15); /* 使用transform替代scale属性 */
    font-size: 18px;
}


</style>