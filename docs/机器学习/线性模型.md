# 线性模型

## 线性可分性

令 $X_0$ 和 $X_1$ 表示 $n$ 维欧式空间的两个点集，如果存在 $n+1$ 个实数 $w_1, w_1, \ldots, w_n, b$，使得对于任意 $X_0$ 中的点 $x$，有 $\sum_{i=1}^{n} x_iw_i + b > 0$，对于任意 $X_1$ 中的点 $x$，有 $\sum_{i=1}^{n} x_iw_i + b < 0$，那么称 $X_0$ 和 $X_1$ 是线性可分的。

## 感知机

感知机由美国学者 $\mathrm{Frank~Rosenblatt}$ 在 $1957$ 年提出，是一种最简单的神经网络模型，是**神经网络和支持向量机的基础**。它是一个典型的**二分类的线性分类模型**，也属于**判别模型**。

即找到一个超平面线性方程 $\mathbf{w}^T\mathbf{x} + b = 0$，将样本空间划分为两个部分，分别为正类和负类。其中 $\mathbf{w}$ 是法向量，与 $\mathbf{x}$ 的维度相同；$b$ 是位移项，决定了超平面的位置。那么感知机的模型可以表示为（其中 $\mathrm{sign}$ 是符号函数）：

$$
f(\mathbf{x}) = \mathrm{sign}(\mathbf{w}^T\mathbf{x} + b)
$$

因此，当 $\mathbf{w}^T\mathbf{x} + b > 0$ 时，$\mathbf{x}$ 被划分为正类；当 $\mathbf{w}^T\mathbf{x} + b < 0$ 时，$\mathbf{x}$ 被划分为负类。这样，我们就完成了分类工作。

## 线性模型

试图去学得一个通过属性的线性组合来进行分类的分类函数，其向量形式为 $f(\mathbf{x}) = \mathbf{w}^T\mathbf{x} + b$，其中 $\mathbf{w}$ 和 $b$ 是模型参数。线性模型的特点是**简单、基本和可理解性好**。但是，线性模型只能适用于**线性可分的**数据集。

### 线性回归

线性回归的求解目标是优化参数，使得预测值接近真实值。即优化 $\mathbf{w}$ 和 $b$，使得 $\mathrm{w^Tx}+b \approx y$。一个衡量标准是均方误差，即目标是让均方误差最小。对均方误差进行求导，可以得到闭式解。

求导的一个技巧是把所有的样本放在一个矩阵中，矩阵中的每一行都是一个样本，并在矩阵后面加上一列全为 $1$ 的列，记为 $\mathbf{X}$，这样就可以把 $b$ 合并到 $\mathbf{w}$ 中，记为 $\mathbf{\hat{w}}$，如下所示：

$$

\mathbf{X}=\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1n} & 1 \\
x_{21} & x_{22} & \cdots & x_{2n} & 1 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
x_{m1} & x_{m2} & \cdots & x_{mn} & 1 \\
\end{bmatrix}
,
\mathbf{\hat{w}}=\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_n \\
b
\end{bmatrix}

$$

这样，线性回归的目标就变成了优化 $\mathbf{\hat{w}}$，使得 $\mathbf{X}\mathbf{\hat{w}} \approx \mathbf{y}$，其中 $\mathbf{y}$ 是真实值。下面，我们定义损失函数为均方误差（系数 $1/2$ 是为了使求导后的结果简洁），即：

$$
\mathbf{\hat{w}}^* = \arg\min_{\mathbf{\hat{w}}} \frac{1}{2}||\mathbf{X}\mathbf{\hat{w}} - \mathbf{y}||^2_2
$$

对损失函数求导，得到

$$
\frac{\partial}{\partial \mathbf{\hat{w}}} \left ( \frac{1}{2}||\mathbf{X}\mathbf{\hat{w}} - \mathbf{y}||^2 \right ) = \mathbf{X}^T(\mathbf{X}\mathbf{\hat{w}} - \mathbf{y})
$$

::: details 推导过程

设 $E_{\mathbf{\hat{w}}}=\frac{1}{2}||\mathbf{X}\mathbf{\hat{w}} - \mathbf{y}||^2$，我们要求 $E_{\mathbf{\hat{w}}}$ 的最小值，即令 $\frac{\partial E_{\mathbf{\hat{w}}}}{\partial \mathbf{\hat{w}}}=0$。将 $E_{\mathbf{\hat{w}}}$ 展开，我们有：

$$
\begin{aligned}
E_{\mathbf{\hat{w}}} &= \frac{1}{2}(\mathbf{X}\mathbf{\hat{w}} - \mathbf{y})^T(\mathbf{X}\mathbf{\hat{w}} - \mathbf{y}) \\
&= \frac{1}{2}\left (\mathbf{\hat{w}}^T\mathbf{X}^T\mathbf{X}\mathbf{\hat{w}}-\mathbf{\hat{w}}^T\mathbf{X}^T\mathbf{y} - \mathbf{y}\mathbf{X}\mathbf{\hat{w}}+\mathbf{y}^T\mathbf{y}\right)\\
\end{aligned}
$$

对 $\mathbf{\hat{w}}$ 求导，我们有：

$$
\begin{aligned}
    \frac{\partial E_{\hat{\mathbf{w}}}}{\partial \hat{\mathbf{w}}}&=\frac{1}{2}\left [\left( \mathbf{X}^T\mathbf{X}\hat{\mathbf{w}}+(\mathbf{X}^T\mathbf{X})^T\hat{\mathbf{w}}\right)-\mathbf{X}^T\mathbf{y}-\mathbf{X}^T\mathbf{y}\right]\\
\end{aligned}
$$

这一步我们用到了标量对向量的求导公式，可以参见[这篇文章](/机器学习/数学基础补充.md#标量对向量)。由于 $\mathbf{X}^T\mathbf{X}$ 是对称矩阵，所以 $(\mathbf{X}^T\mathbf{X})^T=\mathbf{X}^T\mathbf{X}$，所以我们有：

$$

\begin{aligned}
\frac{\partial E_{\hat{\mathbf{w}}}}{\partial \hat{\mathbf{w}}}&=\mathbf{X}^T\mathbf{X}\hat{\mathbf{w}}-\mathbf{X}^T\mathbf{y}\\
&=\mathbf{X}^T(\mathbf{X}\hat{\mathbf{w}}-\mathbf{y})
\end{aligned}
$$

:::

令导数为 $0$，若 $\mathbf{X}^T\mathbf{X}$ 正定，则得到**闭式解**：

$$
\mathbf{\hat{w}}^* = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$

如果我们令 $\ln{y} = \mathbf{w}^T\mathbf{x} + b$，那么线性回归就变成了**对数线性回归**，在某些情况下，对数线性回归会更加贴近真实的标记。

再者，我们继续推广，令某单调可微的连续函数 $g$，那么线性回归就变成了**广义线性回归**，其中 $g$ 被称为**联系函数**。那么，广义线性回归的预测函数为：

$$
f(\mathbf{x}) = g(\mathbf{w}^T\mathbf{x} + b) \quad
g^{-1}(f(\mathbf{x})) = \mathbf{w}^T\mathbf{x} + b
$$

### 二分类任务

线性回归模型产生的实值输出的范围可能是 $(-\infty, +\infty)$，而二分类任务的标记是 $0$ 和 $1$。一个简单的想法是利用联系函数 $g$，将实值输出映射到 $[0, 1]$ 区间。常用的联系函数有：
- **Sigmoid 函数**：$g(z) = \frac{1}{1 + e^{-z}}$，其中 $z = \mathbf{w}^T\mathbf{x} + b$；
- **单位阶跃函数**：$g(z) = \left\{ \begin{array}{ll} 1, & z \geq 0 \\0.5, & z=0\\ 0, & z < 0 \end{array} \right.$；

其中，$\mathrm{Sigmoid}$ 函数是一个常用的联系函数，它的**导数形式简单，且具有良好的性质**。下面我们会继续对它进行开发。

### 对数几率回归（对率回归）

对数几率回归是一种**广义线性回归**，它的联系函数正是 $\mathrm{Sigmoid}$ 函数。虽然名字叫做回归，但对率回归其实是一种**分类学习方法**。并且它不光仅仅输出类别，还会给出近似的概率预测。对数几率回归的模型可以表示为：

$$
y = \frac{1}{1 + e^{-(\mathbf{w}^T\mathbf{x} + b)}}
$$

根据对数的性质，我们可以得到：

$$
\ln{\frac{y}{1 - y}} = \mathbf{w}^T\mathbf{x} + b
$$

将 $\ln{\frac{y}{1 - y}}$ 称为**对数几率**，它表示了正类的概率与负类的概率之比，即：

$$
\ln{\frac{p(y=1|\mathbf{x})}{p(y=0|\mathbf{x})}} = \mathbf{w}^T\mathbf{x} + b
$$

对数几率回归的**损失函数**是**极大似然函数**的负对数（加一个负号转化为最小化问题），对于数据集 $\{(x_i,y_i)\}_{i=1}^{m}$ 即：

$$

\begin{aligned}
\mathcal{L}(\mathbf{w}, b) &= -\ln{\prod_{i=1}^{m} p(y_i|\mathbf{x}_i;~\mathbf{w}, b)} \\
&= -\sum_{i=1}^{m} \ln{p(y_i|\mathbf{x}_i;~\mathbf{w}, b)} 
\end{aligned}

$$

其中，$\mathbf{w}, b$ 相当于是参数，而 $p(y_i|\mathbf{x}_i;~\mathbf{w}, b)$ 是对数几率回归的预测值，也就是说，要令每个样本的预测值尽可能接近真实值。关于似然，可以参考另一篇文章。

接下来，我们进一步化简可以得到：

$$
\mathcal{l}(\mathbf{\hat{w}})=\sum_{i=1}^{m}\left ( -y_i\mathbf{\hat{w}}^T\mathbf{\hat{x}}_i+\ln{(1 + e^{\mathbf{\hat{w}}^T\mathbf{\hat{x}}_i})}\right )
$$

::: details 推导过程

显然，对于似然项，我们可以写成综合两种预测情况的形式：

$$
\begin{aligned}
p(y_i|\mathbf{x}_i;~\mathbf{w}, b) &=p(y_i=1|\mathbf{\hat{x}};\mathbf{\hat{w}})^{y_i}\\&\times p(y_i=0|\mathbf{\hat{x}};\mathbf{\hat{w}})^{1-y_i}
\end{aligned}
$$

那么，代入原式，我们可以得到：

$$
\begin{aligned}
\mathcal{L}(\mathbf{\hat{w}}) &= -\ln{\prod_{i=1}^{m} p(y_i|\mathbf{x}_i;~\mathbf{w}, b)} 
\\&= -\sum_{i=1}^{m} \ln{p(y_i|\mathbf{x}_i;~\mathbf{w}, b)}
\\&= -\sum_{i=1}^{m} \ln{\left (p(y_i=1|\mathbf{\hat{x}};\mathbf{\hat{w}})^{y_i}p(y_i=0|\mathbf{\hat{x}};\mathbf{\hat{w}})^{1-y_i}\right )}
\\&= -\sum_{i=1}^{m} \left [y_i\ln{p(y_i=1|\mathbf{\hat{x}};\mathbf{\hat{w}})}+(1-y_i)\ln{p(y_i=0|\mathbf{\hat{x}};\mathbf{\hat{w}})}\right ]
\\&= -\sum_{i=1}^{m} \left [y_i \ln{\left( \frac{p(y_i=1|\mathbf{\hat{x}};\mathbf{\hat{w}})}{p(y_i=0|\mathbf{\hat{x}};\mathbf{\hat{w}})}\right)} + \ln (p(y_i=0|\mathbf{\hat{x}};\mathbf{\hat{w}})) \right ]
\\&= -\sum_{i=1}^{m} \left [y_i \mathbf{\hat{w}}^T\mathbf{\hat{x}}_i - \ln (1 + e^{\mathbf{\hat{w}}^T\mathbf{\hat{x}}_i}) \right ]
\end{aligned}
$$

最后一步的推导用到了对数几率的定义，即 $\ln{\frac{p(y_i=1|\mathbf{\hat{x}};\mathbf{\hat{w}})}{p(y_i=0|\mathbf{\hat{x}};\mathbf{\hat{w}})}} = \mathbf{\hat{w}}^T\mathbf{\hat{x}}_i$ 和 $p(y_i=0|\mathbf{\hat{x}};\mathbf{\hat{w}})+p(y_i=1|\mathbf{\hat{x}};\mathbf{\hat{w}})=1$。综合两式就可以得到

$$
\begin{aligned}
p(y_i=0|\mathbf{\hat{x}};\mathbf{\hat{w}}) = \frac{1}{1 + e^{\mathbf{\hat{w}}^T\mathbf{\hat{x}}_i}}\\
p(y_i=1|\mathbf{\hat{x}};\mathbf{\hat{w}}) = \frac{e^{\mathbf{\hat{w}}^T\mathbf{\hat{x}}_i}}{1 + e^{\mathbf{\hat{w}}^T\mathbf{\hat{x}}_i}}
\end{aligned}
$$
在最后一步代入即可。$\Box$

:::

### 线性判别分析（Linear Discriminant Analysis, LDA）

将样例投影到一条直线（或低维空间）上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离。也被称为一种**监督降维**技术。

例如，给定数据集 $\{(x_i,y_i)\}_{i=1}^{m}$，设第 $i$ 类样本的集合为 $X_i$，第 $i$ 类样本的均值为 $\mu_i$，第 $i$ 类样本的协方差矩阵为 $\Sigma_i$，那么 $0,1$ 两类样本的中心在直线上的投影点为 $\mathbf{w}^T\mu_0$ 和 $\mathbf{w}^T\mu_1$，两类样本的协方差为 $\mathbf{w}^T\Sigma_0\mathbf{w}$ 和 $\mathbf{w}^T\Sigma_1\mathbf{w}$，那么我们的目标是以下两点：

- 同类样例的投影点尽可能接近，即 $\mathbf{w}^T\Sigma_0\mathbf{w} + \mathbf{w}^T\Sigma_1\mathbf{w}$ 尽可能小；
- 异类样例的投影点尽可能远离，即 $||\mathbf{w}^T\mu_0 - \mathbf{w}^T\mu_1||_{2}^{2}$ 尽可能大。

::: details 推导过程

我们来证明在 $\mathbf{w}$ 下，第 $i$ 类预测的样本投影到直线 $\mathbf{w}$ 后的方差为 $\mathbf{w}^T\Sigma\mathbf{w}$，其中 $\Sigma$ 是第 $i$ 类样本的协方差矩阵。假设第 $i$ 类样本的集合为 $X$，均值为 $\mu$。

对于投影后的每一个样本 $\mathbf{x}_i \in X$ 有 $y_i=\mathbf{w}^T\mathbf{x}_i$ 。那么投影后的样本的均值为 $\tilde{\mu} = \frac{1}{n}\sum_{i=1}^{n}y_i = \mathbf{w}^T\mu$，方差为：

$$
\begin{aligned}
    \mathrm{var}(y) &= \frac{1}{n}\sum_{i=1}^{n}(y_i - \tilde{\mu})^2 
    \\&= \frac{1}{n}\sum_{i=1}^{n}(\mathbf{w}^T\mathbf{x}_i - \mathbf{w}^T\mu)^2 
    \\&= \frac{1}{n}\sum_{i=1}^{n}\left [\mathbf{w}^T(\mathbf{x}_i - \mu)(\mathbf{x}_i - \mu)^T\mathbf{w} \right]
    \\&= \mathbf{w}^T\left (\frac{1}{n}\sum_{i=1}^{n}(\mathbf{x}_i - \mu)(\mathbf{x}_i - \mu)^T\right )\mathbf{w}
    \\&= \mathbf{w}^T\Sigma\mathbf{w}
\end{aligned}
$$

对于协方差，可以参考[这篇文章](/机器学习/数学基础补充.md#协方差)。

$\Box$
:::

<div style="display: flex; align-items: center; justify-content: center; flex-direction: column; padding-top:10px">
    <img src='/image/Screen Shot 2024-09-23 at 1.15.05 PM.png' alt="" style="width:50%;"></img>
    <p style="font-size: 12px; color: gray;">同类接近 异类远离</p>
</div>

因此，我们可以定义**广义瑞利商**：

$$

\begin{aligned}
J(\mathbf{w}) &= \frac{||\mathbf{w}^T\mu_0 - \mathbf{w}^T\mu_1||_{2}^{2}}{\mathbf{w}^T\Sigma_0\mathbf{w} + \mathbf{w}^T\Sigma_1\mathbf{w}} \\
&= \frac{\mathbf{w}^T(\mu_0 - \mu_1)(\mu_0 - \mu_1)^T\mathbf{w}}{\mathbf{w}^T(\Sigma_0 + \Sigma_1)\mathbf{w}}\\
&= \frac{\mathbf{w}^TS_b\mathbf{w}}{\mathbf{w}^TS_w\mathbf{w}}
\end{aligned}

$$

其中，$S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T$ 是**类间散度矩阵**，$S_w = \Sigma_0 + \Sigma_1$ 是**类内散度矩阵**。显然，它们都是**对称矩阵**。

对于 $\mathbf{w}$ 而言，我们可以利用拉格朗日乘子法，使得 $J(\mathbf{w})$ 最大化，最后得到 $\mathbf{w}$ 的解为 $S_w^{-1}(\mu_0 - \mu_1)$。（在这一模型中，我们只关系 $\mathbf{w}$ 的方向，相当于投影直线的走向）这样，我们就得到了一个闭式解。在实际应用中，通常对 $S_w$ 进行奇异值分解，然后得到 $S_w^{-1}$。

::: details 推导过程

由于 $J(\mathbf{w})$ 的分子和分母都是关于 $\mathbf{w}$ 的二次函数，因此结果与 $\mathbf{w}$ 的模长无关（为什么？）。因此，我们不妨固定分母，最大化分子，即

$$
\begin{aligned}
    \min_{\mathbf{w}} &\quad -\mathbf{w}^TS_b\mathbf{w}\\
    \text{s.t.} &\quad \mathbf{w}^TS_w\mathbf{w} = 1
\end{aligned}
$$

下面我们使用拉格朗日乘子法，定义拉格朗日函数：

$$
L(\mathbf{w}, \lambda) = - \mathbf{w}^TS_b\mathbf{w} + \lambda(\mathbf{w}^TS_w\mathbf{w} - 1)
$$

对 $\mathbf{w}$ 求导，我们有：

$$
\begin{aligned}
    \frac{\partial L}{\partial \mathbf{w}} &= -\frac{\partial(\mathbf{w}^TS_b\mathbf{w})}{\partial \mathbf{w}}+\lambda\frac{\partial(\mathbf{w}^TS_w\mathbf{w}-1)}{\partial \mathbf{w}}
    \\&=-(S_b + S_b^T)\mathbf{w} + \lambda(S_w + S_w^T)\mathbf{w}
    \\&=-2S_b\mathbf{w} + 2\lambda S_w\mathbf{w}
\end{aligned}

$$

令导数为 $0$，我们有：

$$
\begin{aligned}
    S_b\mathbf{w} &= \lambda S_w\mathbf{w}
\\ (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T\mathbf{w}&=\lambda S_w\mathbf{w}
\\ (\mu_0 - \mu_1)\gamma &= \lambda S_w\mathbf{w}
\\ \mathbf{w} &\parallel S_w^{-1}(\mu_0 - \mu_1)
\end{aligned}

$$

倒数第二步，我们定义 $\gamma = (\mu_0 - \mu_1)^T\mathbf{w}$，这是一个标量。最后一步，我们就得到了 $\mathbf{w}$ 的方向。$\Box$

:::

## 多分类任务

对于多分类任务，我们可以使用**一对多**的策略，即将多分类任务转化为多个二分类任务。主要分为两种策略，一种是**一对多**，另一种是**一对一**。

- **一对多**：对于 $K$ 个类别，我们训练 $K$ 个分类器，第 $i$ 个分类器将第 $i$ 类作为正类，其他类作为负类。在测试时，我们选择分类器输出最高的类别作为预测结果。
- **一对一**：对于 $K$ 个类别，我们训练 $C_{K}^{2}$ 个分类器，每个分类器只区分两个类别。在测试时，我们选择分类器输出最多的类别作为预测结果。

这两种策略的问题在于**类别不平衡**，即某些类别的样本数远远大于其他类别的样本数。因此，我们可以使用**加权损失函数**，即对于每个类别，我们可以设置一个权重，使得每个类别的损失函数对最终的损失函数的贡献相同。（当然，估计类别的权重也比较困难）