# 持续学习

前面介绍过的深度学习模型在图像识别、NLP 和视频分析等领域获得了巨大的成功。但是，当面对**分布差异很大**的序列任务学习，深度学习会出现**灾难性遗忘**，即在学习时对之前学到的知识迅速破坏的现象。这是训练模型时无法接受的。

> [!NOTE] 神经科学的角度
> 
> 人类的大脑是一个持续学习的系统，人类等高级动物也天生具备终生学习的能力。这种能力可以使我们用旧任务的知识来**帮助**新知识的学习，并将新任务知识**整合**进现有的知识体系。
> 
> 生物学和神经科学的研究发现，生物的知识学习系统中存在**稳定性和可塑性**的平衡。稳定性体现于对已学知识的保持，可塑性体现于对新知识的学习。

受此启发，学术界提出了**持续学习**的概念，其研究的主要目标为，对于一个持续到来的**任务流**，如何在学习新任务的同时：
- **保持**已学知识
- **迁移**已有知识，**加速**新任务的学习

这里的任务流保证了**当前任务的数据在后续任务中不可以被模型访问**，这就基本解决了灾难性遗忘的问题。 例如，迁移学习在经典机器学习和深度学习中关注流式序列任务学习中**所有任务的平均表现**，迁移学习中希望在所有学过的任务中**平均最好**，而在多任务学习中则**串行地学习**多类型任务。

## 场景设定

广义的持续学习任务之间完全不同，现阶段很难展开研究。因此，学术界通常研究以下场景的持续学习：

- 任务增量学习：不同时刻到来的数据分属于不同的任务，可以获得当前任务的所有数据。测试推理时，任务的编号**可知**。
- 类别增量学习：不同时刻到来的数据属于同一类型任务的不同类别，随着训练的进行，类别的数量逐渐增加。测试推理时，任务的编号**不可知**。
- **域增量学习**：不同时刻到来的数据分属于不同的任务，但类别空间完全相同，只是数据的域发生了变化。前后两个任务的数据**不满足独立同分布**。

一般来说，对于 $100$ 类别的数据集，一般划分为 $5$ 个任务，每个任务包含 $20$ 个类别；或者 $20$ 个任务，每个任务包含 $5$ 个类别；还可以划分为一个任务 $50$ 个类别，另外十个任务每个包含 $5$ 个类别。对于 $10$ 个类别的数据集，一般划分为 $5$ 个任务，每个任务包含 $2$ 个类别。

> [!NOTE] 常用数据集
> 
> - $10$ 类别数据集：MNIST、CIFAR-10
> - $100$ 类别数据集：CIFAR-100、mini-ImageNet
> - $200$ 类别数据集：Tiny-ImageNet

对于 $T$ 个任务，持续学习的评估指标基于一个 $T\times T$ 的精度矩阵 $\mathbf{R}$，其中 $R_{ij}$ 表示在任务 $i$ 上训练的模型在任务 $j$ 上的性能。对于任务增量学习，$R_{ij}$ 表示在任务 $i$ 上训练的模型在任务 $j$ 上的性能。对于类别增量学习，$R_{ij}$ 表示在任务 $i$ 上训练的模型在任务 $j$ 上的性能。对于域增量学习，$R_{ij}$ 表示在任务 $i$ 上**训练后**的模型在任务 $j$ 上的性能。

显然，可以得到平均精度为 $\frac{1}{T}\sum_{i=1}^{T}R_{Ti}$。我们还可以得到以下指标：

- $\text{BWT}$ (_Backward Transfer_)
    $$
    \text{BWT} = \frac{1}{T-1}\sum_{i=1}^{T-1}(R_{Ti} - R_{ii})
    $$
主要可以刻画**遗忘**的程度，即在学习新任务时，对之前任务的性能提升。
- $\text{FWT}$ (_Forward Transfer_)
    $$
    \text{FWT} = \frac{1}{T-1}\sum_{i=1}^{T-1}(R_{iT} - R_{ii})
    $$
主要可以刻画**迁移**的程度，即在学习新任务时，之前的知识对新任务的性能提升。

## 持续学习方法

### 基于回放的方法

- **采样回放方法**：学习完每个任务后**保存小一部分当前任务的原始样本**在固定大小的内存空间里。选择要保存的原始样本时，应当选择有**辨别性**和**代表性**的样本。代表模型有 $\text{Rainbow Memory}$ 等。
  ::: tip 辨别性和代表性
  - **辨别性**：靠近分类边界的样本，不确定性高。通过对该样本通过施加不同的转换，可以得到多个扰动样本，然后计算模型对多个扰动样本的输出的方差来衡量样本的不确定性。
  - **代表性**：靠近分布中心的样本更加具有代表性。
  :::
- **生成回放方法**：生成回放方法不保存原始样本，而是训练一个能够生成旧类样本的**生成对抗网络**，可以用于在后续学习过程中随时生成旧类样本。代表模型有 $\text{DGR}$ 和 $\text{DGM}$ 等。
- 伪样本回放方法：固定模型参数，把输入的随机噪声作为可学习参数，将随机噪声优化为伪样本。这一做法和**数据蒸馏**有异曲同工之妙。代表的模型有 $\text{CF-IL}$ 等。

### 基于约束的方法

//TODO
- **参数约束方法**：
- **梯度约束方法**：
- **数据约束方法**：