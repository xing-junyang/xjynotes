# 卷积神经网络

## 背景

**浅层学习**：传统**模式识别**的经典模型，需要人工提取特征，然后使用分类器进行分类。这种方法的缺点是特征提取需要大量的人力和时间，而且特征的选择往往是主观的，不一定是最优的。

**深度学习**：是一种**自动提取特征**的、**端到端**的机器学习方法。深度学习的核心是**神经网络**，每一层神经元都可以自动提取特征，然后传递给下一层神经元。随着特征的传递，其表示的层次逐级升高，最终得到高层次的抽象特征，这些特征可以用于分类、回归等任务。

下面要介绍的**卷积神经网络**（Convolutional Neural Network, $\text{CNN}$）是深度学习的一种重要模型，它在**图像处理、语音识别**等领域取得了很大的成功。

## 卷积 Convolution

卷积是一种**数学运算**，它在信号处理、图像处理等领域有着广泛的应用。在卷积神经网络中，卷积是一种**特殊的线性运算**，它可以有效地提取图像的特征。本文介绍的卷积为离散卷积。

::: tip 为什么要使用卷积

考虑一个 $1000\times 1000$ 的图片，如果使用**全连接**神经网络，那么输入层的神经元个数就是 $1000 \times 1000 = 10^6$。如果我们的隐藏层有 $10^6$ 个神经元，那么隐藏层的参数个数就是 $10^6 \times 10^6 = 10^11$，这个参数量是非常**庞大**的，难以计算。而且全连接神经网络对于图像的**平移、旋转**等操作是不变的，这样显然会导致**参数冗余**。

如果我们采取**部分连接**的方式，即每 $100\times 100$ 的区域连接一个输入层神经元，那么隐藏层的参数个数就是 $10^8$ 个，这样，参数量有所缩小，但还是比较大。

如果我们使用**共享参数**学习的方式，假设我们有 $10\times 10$ 的“过滤器”逐个扫描图片 （$100\times100$ 次）进行学习，那么隐藏层的参数个数就是 $10 \times 10 = 100$ 个，参数量大大减少了。

但是这样的话，我们的模型可能严重**欠拟合**，因为参数量太少了。这时我们可以采取多个“过滤器”的方式，将“过滤器“的数量设为 $100$，这样隐藏层的参数个数就是 $100 \times 10 \times 10 = 10^4$ 个，这样，我们期待不同的“过滤器”可以学习到不同的特征，从而**模型的泛化能力和参数数量得到了平衡**。这种“过滤器扫描”的方法经过形式化定义后，就是本文提到的**卷积**。

:::

对于一个彩色图像，它一般是一个形如 $H \times W \times C$ 的三维张量 $f$（即**矩阵**），其中 $H$ 表示高度，$W$ 表示宽度，$C$ 表示通道数（通常为 $3$，即 `Red`、`Green`、`Blue` 三个通道）。而卷积核是一个 $h \times w \times c$ 的三维张量 $g$，通常有 $h\ll H, w \ll W$，而 $c$ 与输入张量的通道数 $C$ 相同。

卷积处理的结果也是一个矩阵，记为 $f*g$，我们有：

$$
(f*g)_{i,j} = \sum_{u=1}^h \sum_{v=1}^w \sum_{k=1}^c f_{i+u-1, j+v-1, k} \cdot g_{u, v, k}
$$

其中 $1 \leq i \leq H-h+1, 1 \leq j \leq W-w+1$。我们可以发现，经过卷积运算后，输出的矩阵的高度和宽度都会**缩小**。

> [!NOTE] 如何理解卷积运算？
> 
> 我们可以想象把卷积核嵌入原始矩阵的某个位置上（**对其网格**哦），然后卷积核上的元素就会与原始矩阵上的元素“**重叠**”，将重叠的元素**相乘**，并将它们**求和**，就得到了卷积核在**这个位置**（即卷积核**中心**）上的输出值。把卷积核在整个矩阵上移动，遍历所有可能的位置，保持输出值的**相对位置不变**，我们就得到了输出矩阵。
>
>
> <div style="display: flex; align-items: center; justify-content: center; flex-direction: column; padding-top:10px">
>    <img src='/image/conv_orig.jpeg' alt="" style="width:60%;"></img>
>    <p style="font-size: 12px; color: gray;">卷积的直观展示</p>
> </div>

## 池化 Pooling

池化是一种**非线性下采样**的操作，它可以减少数据量，同时保留重要的特征。池化操作通常在卷积层之后，可以减少特征的数量，从而减少计算量，并最大程度上保留特征。

池化操作的过程是这样的：对于一个 $H \times W \times C$ 的张量，我们可以使用一个 $h \times w$ 的池化窗口，然后对窗口中的元素进行**池化操作**，最终得到一个 $H/h \times W/w \times C$ 的张量。有些类似图像的**缩小**操作。

常见的池化操作有**最大池化**和**平均池化**。顾名思义，最大池化是取池化窗口中的最大值，平均池化是取池化窗口中的平均值。

## 卷积神经网络

卷积神经网络的主要结构包括**卷积层**、**池化层**、**全连接层**和 $\text{Softmax}$ 层。卷积层和池化层用于**充分提取图像的特征并减少参数量**，全连接层用于**分类**，而之后的 $\text{Softmax}$ 层用于**输出分类结果**。卷积层和池化层可以**多次交替**出现，以提取更高层次的特征。

之后，串联起整个卷积神经网络，我们发现它实质上是一个输入为高维数据（如图像）、输出为类别的**模型**。因此，我们依然可以使用上一节介绍的**有监督的学习机制**和[**反向传播算法**](./神经网络.md#反向传播算法)来训练卷积神经网络。

::: tip 为何可以使用 BP 算法

卷积神经网络虽然结构复杂，但是它实质上仍然是一个复合函数。我们可以将卷积神经网络，从**中间的任意一层**、**到最终的损失函数输出**，看成一个**复合函数**。此时，我们将输入看作是**参数**，而该层的参数看作是**自变量**，将损失函数看作是**因变量**，我们可以计算出**损失函数对参数的梯度**（由于**链式法则**，计算时很可能用到下一层的梯度，所以使用**反向传播**），然后使用**梯度下降**等方法来**更新参数**。

:::

## 优化

### Dropout

Dropout 是一种**正则化**的方法，它可以减少**过拟合**。Dropout 的原理是在**训练**过程中，**随机**地让一部分神经元**失活**，这样可以减少神经元之间的**依赖性**，从而减少过拟合。

### Batch Normalization

Batch Normalization 也是一种**正则化**的方法，它的思想是在每一个**小批量**的数据上，对**每一层**的输入进行**归一化**，使得每一层的输入的均值为 $0$，方差为 $1$。这样可以**加速训练**，**减少梯度消失**，同时也可以减少某些数据造成的**影响**。

> [!NOTE] Batch Normalization 的流程
> 
> 对于一个小批量数据 $B = \{x_1, x_2, \cdots, x_m\}$，我们可以计算出在某一层上的均值 $\mu_B$ 和方差 $\sigma_B$，然后对每一个输入 $x_i$ 进行归一化：
> 
> $$
> \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
> $$
> 
> 其中 $\epsilon$ 是一个很小的数，避免分母为 $0$。然后我们对 $\hat{x}_i$ 进行缩放和平移：
> 
> $$
> y_i = \gamma \hat{x}_i + \beta
> $$
> 
> 其中 $\gamma$ 和 $\beta$ 是**可学习的参数**，可以放入神经网络的参数中**一起学习**。

