# 卷积神经网络和深度学习

## 背景

**浅层学习**：传统**模式识别**的经典模型，需要人工提取特征，然后使用分类器进行分类。这种方法的缺点是特征提取需要大量的人力和时间，而且特征的选择往往是主观的，不一定是最优的。

**深度学习**：是一种**自动提取特征**的、**端到端**的机器学习方法。深度学习的核心是**神经网络**，每一层神经元都可以自动提取特征，然后传递给下一层神经元。随着特征的传递，其表示的层次逐级升高，最终得到高层次的抽象特征，这些特征可以用于分类、回归等任务。

下面要介绍的**卷积神经网络**（Convolutional Neural Network, $\text{CNN}$）是深度学习的一种重要模型，它在**图像处理、语音识别**等领域取得了很大的成功。

## 卷积 Convolution

卷积是一种**数学运算**，它在信号处理、图像处理等领域有着广泛的应用。在卷积神经网络中，卷积是一种**特殊的线性运算**，它可以有效地提取图像的特征。本文介绍的卷积为离散卷积。

::: tip 为什么要使用卷积

考虑一个 $1000\times 1000$ 的图片，如果使用**全连接**神经网络，那么输入层的神经元个数就是 $1000 \times 1000 = 10^6$。如果我们的隐藏层有 $10^6$ 个神经元，那么隐藏层的参数个数就是 $10^6 \times 10^6 = 10^11$，这个参数量是非常**庞大**的，难以计算。而且全连接神经网络对于图像的**平移、旋转**等操作是不变的，这样显然会导致**参数冗余**。

如果我们采取**部分连接**的方式，即每 $100\times 100$ 的区域连接一个输入层神经元，那么隐藏层的参数个数就是 $10^8$ 个，这样，参数量有所缩小，但还是比较大。

如果我们使用**共享参数**学习的方式，假设我们有 $10\times 10$ 的“过滤器”逐个扫描图片 （$100\times100$ 次）进行学习，那么隐藏层的参数个数就是 $10 \times 10 = 100$ 个，参数量大大减少了。

但是这样的话，我们的模型可能严重**欠拟合**，因为参数量太少了。这时我们可以采取多个“过滤器”的方式，将“过滤器“的数量设为 $100$，这样隐藏层的参数个数就是 $100 \times 10 \times 10 = 10^4$ 个，这样，我们期待不同的“过滤器”可以学习到不同的特征，从而**模型的泛化能力和参数数量得到了平衡**。这种“过滤器扫描”的方法经过形式化定义后，就是本文提到的**卷积**。

:::

对于一个彩色图像，它一般是一个形如 $H \times W \times C$ 的三维张量 $f$（即**矩阵**），其中 $H$ 表示高度，$W$ 表示宽度，$C$ 表示通道数（通常为 $3$，即 `Red`、`Green`、`Blue` 三个通道）。而卷积核是一个 $h \times w \times c$ 的三维张量 $g$，通常有 $h\ll H, w \ll W$，而 $c$ 与输入张量的通道数 $C$ 相同。

卷积处理的结果也是一个矩阵，记为 $f*g$，我们有：

$$
(f*g)_{i,j} = \sum_{u=1}^h \sum_{v=1}^w \sum_{k=1}^c f_{i+u-1, j+v-1, k} \cdot g_{u, v, k}
$$

其中 $1 \leq i \leq H-h+1, 1 \leq j \leq W-w+1$。我们可以发现，经过卷积运算后，输出的矩阵的高度和宽度都会**缩小**。

> [!NOTE] 如何理解卷积运算？
> 
> 我们可以想象把卷积核嵌入原始矩阵的某个位置上（**对其网格**哦），然后卷积核上的元素就会与原始矩阵上的元素“**重叠**”，将重叠的元素**相乘**，并将它们**求和**，就得到了卷积核在**这个位置**（即卷积核**中心**）上的输出值。把卷积核在整个矩阵上移动，遍历所有可能的位置，保持输出值的**相对位置不变**，我们就得到了输出矩阵。
>
>
> <div style="display: flex; align-items: center; justify-content: center; flex-direction: column; padding-top:10px">
>    <img src='/image/conv_orig.jpeg' alt="" style="width:60%;"></img>
>    <p style="font-size: 12px; color: gray;">卷积的直观展示</p>
> </div>

## 池化 Pooling

池化是一种**非线性下采样**的操作，它可以减少数据量，同时保留重要的特征。池化操作通常在卷积层之后，可以减少特征的数量，从而减少计算量，并最大程度上保留特征。

池化操作的过程是这样的：对于一个 $H \times W \times C$ 的张量，我们可以使用一个 $h \times w$ 的池化窗口，然后对窗口中的元素进行**池化操作**，最终得到一个 $H/h \times W/w \times C$ 的张量。有些类似图像的**缩小**操作。

常见的池化操作有**最大池化**和**平均池化**。顾名思义，最大池化是取池化窗口中的最大值，平均池化是取池化窗口中的平均值。

## 卷积神经网络

卷积神经网络的主要结构包括**卷积层**、**池化层**、**全连接层**和 $\text{Softmax}$ 层。卷积层和池化层用于**充分提取图像的特征并减少参数量**，全连接层用于**分类**，而之后的 $\text{Softmax}$ 层用于**输出分类结果**。卷积层和池化层可以**多次交替**出现，以提取更高层次的特征。

之后，串联起整个卷积神经网络，我们发现它实质上是一个输入为高维数据（如图像）、输出为类别的**模型**。因此，我们依然可以使用上一节介绍的**有监督的学习机制**和[**反向传播算法**](./神经网络.md#反向传播算法)来训练卷积神经网络。

::: tip 为何可以使用 BP 算法

卷积神经网络虽然结构复杂，但是它实质上仍然是一个复合函数。我们可以将卷积神经网络，从**中间的任意一层**、**到最终的损失函数输出**，看成一个**复合函数**。此时，我们将输入看作是**参数**，而该层的参数看作是**自变量**，将损失函数看作是**因变量**，我们可以计算出**损失函数对参数的梯度**（由于**链式法则**，计算时很可能用到下一层的梯度，所以使用**反向传播**），然后使用**梯度下降**等方法来**更新参数**。

:::

## 优化

### Dropout

Dropout 是一种**正则化**的方法，它可以减少**过拟合**。Dropout 的原理是在**训练**过程中，**随机**地让一部分神经元**失活**，这样可以减少神经元之间的**依赖性**，从而减少过拟合。

### Batch Normalization

Batch Normalization 也是一种**正则化**的方法，它的思想是在每一个**小批量**的数据上，对**每一层**的输入进行**归一化**，使得每一层的输入的均值为 $0$，方差为 $1$。这样可以**加速训练**，**减少梯度消失**，同时也可以减少某些数据造成的**影响**。

> [!NOTE] Batch Normalization 的流程
> 
> 对于一个小批量数据 $B = \{x_1, x_2, \cdots, x_m\}$，我们可以计算出在某一层上的均值 $\mu_B$ 和方差 $\sigma_B$，然后对每一个输入 $x_i$ 进行归一化：
> 
> $$
> \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
> $$
> 
> 其中 $\epsilon$ 是一个很小的数，避免分母为 $0$。然后我们对 $\hat{x}_i$ 进行缩放和平移：
> 
> $$
> y_i = \gamma \hat{x}_i + \beta
> $$
> 
> 其中 $\gamma$ 和 $\beta$ 是**可学习的参数**，可以放入神经网络的参数中**一起学习**。

下面是一个**使用 $\text{VGG16}$ 模型在 $\text{Caltech 101}$ 训练的简单例子**，读者可以下载查看。

<button class="button-download" onclick="window.open('/pdf/vgg16_pdf.pdf')">下载 PDF</button> <button class="button-download" style="background-color:#aaaaaa" onclick="window.open('/pdf/vgg16_code.zip')">下载代码（.zip）</button>

<style>
.button-download {
    background-color: #42b983; /* Green */
    border: none;
    color: white;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    font-weight: bold;
    cursor: pointer;
    border-radius: 20px;
    height: 40px;
    line-height: 40px;
    padding: 0 20px;
    margin-left: 25px;
    transition: all 0.3s ease; /* 添加transition效果 */
}

.button-download:hover {
    background-color: #00e175;
    font-weight: 800;
    transform: scale(1.15); /* 使用transform替代scale属性 */
    font-size: 18px;
}
</style>

## 深度学习的技巧

> [!IMPORTANT] 自己搭建一个简单的深度学习模型
> 
> “**纸上得来终觉浅**”。我们不妨自己动手搭建一个简单的深度学习模型。这样可以更好地理解深度学习的原理，也可以更好地掌握深度学习的技巧。
> 
> 一个好的上手教程是：[Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)。这个教程搭建了一个简单的图像分类模型，并在 CIFAR-10 数据集上进行训练，是一个很好的入门教程。

- **数据增广**：数据增广是一种**数据扩充**的方法，它可以通过对原始数据进行**旋转、翻转、裁切、加噪音**等操作，生成更多的数据，从而**增加训练集**的大小，减少过拟合。
- **预处理**：`PCA` 白化、`Zero Center`（减去均值） 和 `Normalize`（放缩到  1 内） 等。
- **初始化**：增加随机性，`Calibration`（使得输出的方差和输入的方差相等）等。
- **过滤器**：卷积核的大小、步长、填充方式等。
- **池化大小**
- **学习率**：Step attenuation（学习率逐渐减小）、调优时选择小的学习率等。
- **在预训练模型上进行微调**

::: tip 对于不同的数据集，使用不同策略！

我们经常会遇到不同的数据集，而不同的数据集可能需要相应的策略才能达到好的训练效果。经验上，我们可以总结如下：

|       | 数据集较相似      | 数据集较不相似           |
|-------|-------------|-------------------|
| 数据集较小 | 尝试第一层使用线性模型 | 较难处理，可以在不同层尝试线性模型 |
| 数据集较大 | 浅层神经网络      | 深层神经网络            |

:::

- 激活函数和正则化：激活函数如 `ReLU`、`Leaky ReLU`、`ELU` 等，正则化如 `L1`、`L2` 正则化、[`Dropout`](./卷积神经网络.md#dropout)、[`Batch Normalization`](./卷积神经网络.md#batch-normalization) 等。
- **图像分析与可视化**：可以观察学习的准确率曲线、损失函数曲线等，以判断模型的训练效果。此外，我们还可以可视化中间层的参数等，以体现出模型的特征提取能力。

::: tip 选择哪个 `Epoch` 的参数？

一般我们把在**验证集**上表现最好的参数作为最终的参数。如果我们的模型在**验证集**和**训练集**上表现差异很大，那么我们的模型很可能**过拟合**了。如果我们的模型在**验证集**和**训练集**上表现都不好，那么就应该发生了**欠拟合**。

:::

> [!NOTE] 深度学习工具箱
> 
> - [**PyTorch**](http://pytorch.org/): tensors and dynamic neural networks in python (Python);
> - [**TensorFlow**](https://www.tensorflow.org): An end-to-end platform for machine learning;
> - [**MatConvNet**](http://www.vlfeat.org/matconvnet/): A matlab toolbox implementing CNNs (Matlab);
> - [**Caffe**](http://caffe.berkeleyvision.org/): C++, Python;
> - [**Keras**](https://keras.io/): A theano based deep learning library;
> - [**Torch**](http://torch.ch/): provides a Matlab-like environment for ML algorithms in lua (Lua);
> - [**Theano**](http://deeplearning.net/software/theano/): CPU/GPU symbolic expression compiler in python (Python);
> - [**MXNet**](https://github.com/dmlc/mxnet): mix symbolic and imperative programming (Python, R, Julia...);
> - [**CNTK**](https://docs.microsoft.com/en-us/cognitive-toolkit/): a unified deep-learning toolkit by Microsoft (C++, Python).
> - [**PaddlePaddle**](https://www.paddlepaddle.org.cn/en): An Open-Source Deep Learning Platform (百度)
> - [**MindSpore**](https://www.mindspore.cn/): 面向“端-边-云”全场景设计的AI框架(华为)
> - [**MegEngine**](https://link.zhihu.com/?target=https%3A//megengine.org.cn/): 一个快速、可拓展、易于使用且支持自动求导的深度学习框架(旷视)
> - [**Jittor**](https://cg.cs.tsinghua.edu.cn/jittor/):一个基于即时编译和元算子的高性能深度学习框架(清华大学)
