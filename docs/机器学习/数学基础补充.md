# 数学基础补充

在机器学习领域中，**数学**是非常重要的基石，涉及到了代数学、微积分、概率论、统计学等多个分支。相对于传统的系统化的数学学习，机器学习中的数学学习更加注重于应用，因此也就更加分散和颗粒化。可是，从个人的切身体会来看，对于非数学专业的学生或相关人员来讲，如果没有系统地学习过这些数学分支，这些知识可能看起来并没有那么“基础”。这样看来，我们可以以机器学习的视角出发，快速地填补一些知识漏洞，或者去重温这些数学知识。这篇文章的目标就是集中地整理机器学习中所涉及的数学基础知识。全文以知识点的方式布局，并且假定读者已经具备了大学本科层次的**高等数学和线性代数**的基础知识（即绝大多数理工科专业数学基础课程的知识）。

## 矩阵的导数

在高等数学中，我们学过一元函数 $f(x)$ 的导数的定义，同时也学过多元函数如 $u(x,y)$ 的偏导数的定义。这些导数，我们可以统称为**标量对标量**的导数，因为无论自变量如何，极限式的分子分母始终都是一个数值。

我们在线性代数中还学过向量和矩阵。在实际应用中我们发现，很多运算都可以简记为向量和矩阵的形式，并且它们还有一些良好的性质来方便我们计算。那么，导数的概念是否可以继续推广呢？先来看一个简单的例子。

::: warning 符号说明

- 用大写粗体字母表示矩阵，如 $\mathbf{A}$；用斜体字母和脚标表示矩阵中的元素，如 $A_{ij}$ 或 $a_{ij}$；
- 用小写粗体字母表示向量，如 $\mathbf{x}$；用小写斜体字母和脚标表示向量中的元素，如 $x_i$；如没有特殊说明，约定向量为**列向量**；
- 用小写斜体字母表示标量，如 $f$ 或 $x$。

<!-- 矩阵的转置用 $\mathbf{A}^T$ 表示，矩阵的逆用 $\mathbf{A}^{-1}$ 表示。矩阵的行列式用 $|\mathbf{A}|$ 表示。矩阵的迹用 $\mathrm{tr}(\mathbf{A})$ 表示。矩阵的行用 $\mathbf{A}_{i,:}$ 表示，矩阵的列用 $\mathbf{A}_{:,j}$ 表示。矩阵的元素用 $a_{ij}$ 表示。 -->
:::

**梯度** 梯度是一个**向量**，表示某一多元函数在该点处的方向导数沿着该方向取得最大值。对于一个具有**一阶连续偏导数**的多元函数 $f(\mathbf{x})$（这里，我们把自变量看作一个向量，这也是在机器学习中的惯用做法），我们知道在 $\mathbf{x}_0=(x_1, x_2, \cdots, x_n)^T$ 处的梯度为：

$$
\nabla f(\mathbf{x}) = \left [ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n} \right ]^{T}
$$

这里，$\nabla$ 是**梯度算子**，$\nabla f(\mathbf{x})$ 是 $f(\mathbf{x})$ 的梯度。梯度的每一个分量都是 $f(\mathbf{x})$ 对应的自变量的偏导数。这样，我们不妨把标量对标量的导数推广到**标量对向量**的导数。定义

$$
\frac{\partial f}{\partial \mathbf{x}} = \left [ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n} \right ]^{T}
$$

这样，我们就可以把梯度的公式简写为 
$$
\nabla f(\mathbf{x}) = \frac{\partial f}{\partial \mathbf{x}}
$$

由此我们可以看出，向量和矩阵的求导其实就是对**多元函数求偏导**，只不过是将**自变量、因变量和结果**排列成了向量或矩阵的形式，是一种**简记**的方式。当然，在我们全部定义后，它还可以推出很多方便的性质可用于推导和计算。这就是**矩阵微分**的一大内容。因此，下面我们列出完整的定义。

| 自变量 / 因变量       | 矩阵   $\mathbf{F}$                          | 向量  $\mathbf{f}$                        | 标量  $f$           |
|---------------------|----------------------------------|------------------------------|------------------|
| 矩阵 $\mathbf{X}$  | 不讨论 | 不讨论 | $\frac{\partial f}{\partial \mathbf{X}_{m \times n}} = \left [ \frac{\partial f}{\partial x_{ij}} \right ]_{m\times n}$ |
| 向量 $\mathbf{x}$ | 不讨论 | $\frac{\partial \mathbf{f} }{\partial \mathbf{x}} =\begin{bmatrix}\frac{\partial f_1 }{\partial x_1}&\frac{\partial f_2 }{\partial x_1}  & \frac{\partial f_3 }{\partial x_1} & \cdots  &\frac{\partial f_m }{\partial x_1} \\\frac{\partial f_1 }{\partial x_2}&\frac{\partial f_2 }{\partial x_2}  & \frac{\partial f_3 }{\partial x_2} & \cdots & \frac{\partial f_m }{\partial x_2}\\\frac{\partial f_1 }{\partial x_3}&\frac{\partial f_2 }{\partial x_3}  & \frac{\partial f_3 }{\partial x_3} & \cdots & \frac{\partial f_m }{\partial x_3}\\ \vdots & \vdots &\vdots  &\ddots   & \vdots\\\frac{\partial f_1 }{\partial x_n}&\frac{\partial f_2 }{\partial x_n}  & \frac{\partial f_3 }{\partial x_n} &\cdots &\frac{\partial f_m }{\partial x_n}\end{bmatrix}_{n\times m}$ | $\frac{\partial f} {\partial \mathbf{x}} = \left [ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n} \right ]^{T}$ |
| 标量 $x$  | $\frac{\partial \mathbf{F}_{m\times n}}{\partial x} = \left [ \frac{\partial f_{ji}}{\partial x} \right ]_{n\times m}$         | $\frac{\partial \mathbf{f}}{\partial x}=\left [ \frac{\partial f_1}{\partial x}, \frac{\partial f_2}{\partial x}, \cdots, \frac{\partial f_n}{\partial x} \right ]$  | 平凡 |


::: tip So...列向量还是行向量？

在计算向量或矩阵的导数时，一个问题就是自变量、因变量和结果到底应该怎么排列呢？比如，分母应该是行向量还是列向量？结果呢？前文说过，这只是一个排列方式，似乎一个小小的转置并不重要。但我们最好还是仔细定义来澄清这一点。

一般而言，分为两种模式，即**分子布局**和**分母布局**。它们的选择是随意的，**但在一次求导运算中，只能选择一种布局，不能杂糅**。

上文的表格中，我们使用的就是**分母布局**。它的含义是，**结果的维数必须要与分母的维数或分子的转置的维数相同**。进一步说明：第一，我们只需要一个分子或分母的一个判据即可：也就是说，如果分母为标量，那只能看分子的转置，反之亦然。第二，当选择的判据是向量而结果是矩阵时，若是行向量就只用看矩阵的列数是否等于向量的维数，若是列向量就只看行数。

分子布局与分母布局刚刚相反，两者的差异在于一个**转置**。

但在实际应用中，我们往往会选择**混合布局**，即在一次求导运算中，我们可以选择不同的布局。这样做的好处是，可以减少转置的次数，提高计算效率。但是，这样可能会增加一些混乱，需要我们时刻注意自己选择的布局。
:::

有了上面的说明，就可以利用微分学的知识和一点点代数的能力，推导出很多简明的结论。下面我们给出一些。

### 标量对向量

- $\frac{\partial \mathbf{a^Tx}}{\partial \mathbf{x}} = \mathbf{a}$，其中 $\mathbf{a}$ 是一个常向量；
- $\frac{\partial \mathbf{x^Tx}}{\partial \mathbf{x}} = 2\mathbf{x}$；
- $\frac{\partial \mathbf{x^TAx}}{\partial \mathbf{x}} = \mathbf{A}\mathbf{x}+\mathbf{A}^T\mathbf{x}$，这是因为 $[\frac{\partial \mathbf{x^TAx}}{\partial \mathbf{x}}]_k = \frac{\partial \sum_{i}\sum_{j}x_ix_jA_{ij}}{\partial x_k}= \sum_{i=1}^nx_i(A_{ik}+A_{ki})$
- 若 $f,g$ 都是关于 $\mathbf{x}$ 在对应点可导的标量函数，则 
  - $c_1\frac{\partial f}{\partial \mathbf{x}} + c_2\frac{\partial g}{\partial \mathbf{x}} = \frac{\partial (c_1f(\mathbf{x})+c_2g(\mathbf{x}))}{\partial \mathbf{x}}$；
  - $\frac{\partial f(\mathbf{x})\cdot g(\mathbf{x})}{\partial \mathbf{x}} = f(\mathbf{x})\frac{\partial g}{\partial \mathbf{x}} + g(\mathbf{x})\frac{\partial f}{\partial \mathbf{x}}$；
  - $\frac{\partial f(\mathbf{x})/g(\mathbf{x})}{\partial \mathbf{x}} = \frac{g(\mathbf{x})\frac{\partial f}{\partial \mathbf{x}} - f(\mathbf{x})\frac{\partial g}{\partial \mathbf{x}}}{g^2(\mathbf{x})}$；
  - $\frac{\partial f(g(\mathbf{x}))}{\partial \mathbf{x}} = \frac{\partial f}{\partial g}\frac{\partial g}{\partial \mathbf{x}}$；

这些结论通过学过的微分学知识和矩阵的性质可以推导出来。至于向量对标量的导数，相对而言就更加简单，这里就不再赘述。

### 向量对向量



## 先验概率与后验概率

## 似然