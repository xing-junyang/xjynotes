# 数学基础补充

在机器学习领域中，**数学**是非常重要的基础，涉及到了代数学、微积分、概率论、统计学等多个分支。相对于传统的系统化的数学学习，机器学习中的数学学习更加注重于应用，因此也就更加分散和颗粒化。可是，从个人的切身体会来看，对于非数学专业的学生或相关人员来讲，如果没有系统地学习过这些数学分支，这些知识可能看起来并没有那么“基础”。这样看来，我们可以以机器学习的视角出发，快速地填补一些知识漏洞，或者去重温这些数学知识。这篇文章的目标就是集中地整理机器学习中所涉及的数学基础知识。全文以知识点的方式布局，并且假定读者已经具备了大学本科层次的**高等数学和线性代数**的基础知识（即绝大多数理工科专业数学基础课程的知识）。

## 矩阵的导数

在高等数学中，我们学过一元函数 $f(x)$ 的导数的定义，同时也学过多元函数如 $u(x,y)$ 的偏导数的定义。这些导数，我们可以统称为**标量对标量**的导数，因为无论自变量如何，极限式的分子分母始终都是一个数值。

我们在线性代数中还学过向量和矩阵。在实际应用中我们发现，很多运算都可以简记为向量和矩阵的形式，并且它们还有一些良好的性质来方便我们计算。那么，导数的概念是否可以继续推广呢？先来看一个简单的例子。

::: warning 符号说明

- 用大写粗体字母表示矩阵，如 $\mathbf{A}$；用斜体字母和脚标表示矩阵中的元素，如 $A_{ij}$ 或 $a_{ij}$；
- 用小写粗体字母表示向量，如 $\mathbf{x}$；用小写斜体字母和脚标表示向量中的元素，如 $x_i$；如没有特殊说明，约定向量为**列向量**；
- 用小写斜体字母表示标量，如 $f$ 或 $x$。

<!-- 矩阵的转置用 $\mathbf{A}^T$ 表示，矩阵的逆用 $\mathbf{A}^{-1}$ 表示。矩阵的行列式用 $|\mathbf{A}|$ 表示。矩阵的迹用 $\mathrm{tr}(\mathbf{A})$ 表示。矩阵的行用 $\mathbf{A}_{i,:}$ 表示，矩阵的列用 $\mathbf{A}_{:,j}$ 表示。矩阵的元素用 $a_{ij}$ 表示。 -->
:::

**梯度** 梯度是一个**向量**，表示某一多元函数在该点处的方向导数沿着该方向取得最大值。对于一个具有**一阶连续偏导数**的多元函数 $f(\mathbf{x})$（这里，我们把自变量看作一个向量，这也是在机器学习中的惯用做法），我们知道在 $\mathbf{x}_0=(x_1, x_2, \cdots, x_n)^T$ 处的梯度为：

$$
\nabla f(\mathbf{x}) = \left [ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n} \right ]^{T}
$$

这里，$\nabla$ 是**梯度算子**，$\nabla f(\mathbf{x})$ 是 $f(\mathbf{x})$ 的梯度。梯度的每一个分量都是 $f(\mathbf{x})$ 对应的自变量的偏导数。这样，我们不妨把标量对标量的导数推广到**标量对向量**的导数。定义

$$
\frac{\partial f}{\partial \mathbf{x}} = \left [ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n} \right ]^{T}
$$

这样，我们就可以把梯度的公式简写为 
$$
\nabla f(\mathbf{x}) = \frac{\partial f}{\partial \mathbf{x}}
$$



## 先验概率与后验概率

## 似然